%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% LaTeX book template                           %%
%% Author:  Amber Jain (http://amberj.devio.us/) %%
%% License: ISC license                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[a4paper, 11pt, twoside]{book}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Source: http://en.wikibooks.org/wiki/LaTeX/Hyperlinks %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\usepackage{cite}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage[english]{babel}
\usepackage{float}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{microtype}
\usepackage{times}
\usepackage[utf8]{inputenc}     
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{lipsum}
\usepackage{exercise}
\usepackage{array}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{paralist}
\usepackage[super]{nth}
\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage{amsmath}
\usepackage{pstricks}
\usepackage{tikz}
\usetikzlibrary{positioning, shapes, shapes.geometric, arrows}

\newtheorem{rexample}{R Example}[section]

\newcommand{\aggregate}[2]{\underset{#2}{\operatornamewithlimits{#1\ }}}

\tikzstyle{block1} = [rectangle, draw, fill=blue!20, text centered, rounded corners]
\tikzstyle{block2} = [rectangle, draw, fill=red!20, text centered, rounded corners]
\tikzstyle{block3} = [rectangle, draw, text centered]
\tikzstyle{arrow} = [thick,->,>=stealth]

\SetKw{Break}{break}
\SetKw{Continue}{continue}
\SetKw{Initialise}{Initialise}

\setlength{\parskip}{1em}
 
<<setup, include=FALSE>>=
library(checkpoint)
checkpoint(snapshotDate = "2018-08-01", 
           auto.install.knitr = TRUE, 
           scan.rnw.with.knitr = TRUE, 
           verbose = TRUE,
           use.knitr = TRUE,
           scanForPackages = TRUE,
           checkpointLocation=getwd())

library(knitr)
knit_hooks$set(rexample = function(before, options, envir) {
  if (before) sprintf("\\begin{rexample}\\label{%s}\\hfill{}", options$label) else "\\end{rexample}"
})
@

% Book's title and subtitle
\title{
  \Huge \textbf{Business Analytics in R} \\ 
  \huge Introduction to Statistical Programming
}
% Author
\author{
  \textsc{Timothy Wong}\thanks{\url{timothy.wong@hotmail.co.uk}} \\
  \textsc{James Gammerman}\thanks{\url{jgammerman@gmail.com}}
}

\begin{document}
% \SweaveOpts{concordance=TRUE}
\frontmatter
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Add a dedication paragraph to dedicate your book to someone %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{dedication}
%Dedicated to Calvin and Hobbes.
%\end{dedication}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Auto-generated table of contents, list of figures and list of tables %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\tableofcontents
\listoffigures
\listoftables

\mainmatter

%%%%%%%%%%%
% Preface %
%%%%%%%%%%%
\chapter*{Preface}

R is an open source language for statistical programming. It is widely used among statisticians, academic researchers, and business analysts across the world. This book is part of a training course designed for business analysts.

\section*{Web Access}
The \LaTeX\xspace source code and the compiled PDF version of this book can be digitially accessed at \url{https://github.com/timothywong731/r-training}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Give credit where credit is due. %
% Say thanks!                      %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgements}
\begin{itemize}
\item A special word of thanks goes to the open source R community.
\item Some of the examples in \autoref{ch:datatransformation} were adopted from Wickham and Grolemund's book~\cite{wickham}.
\end{itemize}
% \mbox{}\\
% \mbox{}\\

% \noindent Tim \\
% \noindent \url{timothy.wong@hotmail.co.uk}



\chapter{R Ecosystem}

\verb|R| is a language for statistical programming. The language is open source and free for personal, academic and commerical use. It is available on multiple platforms including Windows, Linux and MacOS. In recent years, \verb|R| has gained popularity and became one of the fastest growing programming languages in the world\footnote{\url{https://stackoverflow.blog/2017/10/10/impressive-growth-r/}}. In this chapter, we will go through the basics of \verb|R|.

\section{Programming Enviornment}

RStudio\footnote{\url{https://www.rstudio.com}} is a popular Integrated Development Environment (IDE) for the \verb|R| language. It is a very powerful editor which enables programmers to interact with the \verb|R| language. RStudio is open-sourced and the desktop version is free for personal use. 

Inside RStudio, the display is divided into different tabs which user can customise. The following ones are particularly important:

\begin{description}
  \item [Source Editor]
  Inside RStudio, users can access the source editor for \verb|R| code. This can be treated as a powerful text editor for various forms of R code, such as standard \verb|R| Script, \verb|R| Markdown, \verb|R| Notebook, \verb|R| Sweave and \verb|shiny| etc. The source editor does not execute any code, it purely helps user edit it efficiently.
  
  \item [Console]
  The \verb|R| interpreter is shown in the Console window. This is the place where \verb|R| code is processed. User can highlight a segment of \verb|R| code in the main text editor and press \verb|Ctrl + Enter| to send it to execution. Once the code is sent, it will execute in the \verb|R| interpreter and the results will be displayed. Alternatively, user can just place the blinking cursor on a line and press the same keys. This sends the entire line to the interpreter for execution. User can press \verb|Ctrl + L| to clear existing screen output.
  
  \item [Environment]
  User can access all the variables created in the current \verb|R| session. The workspace which includes all variables can be exported as \verb|.RData| file in the environment window. User can also import workspace from an existing \verb|.RData| file.
  
  \item [Files]
  It allows users to browse files on the local storage system. User can navigate to their own home directory using the \verb|~| path.
  
  \item [Plots]
  All graphical outputs produced by the \verb|R| interpreter are displayed in the plots tab. Users can export the graph as file in this tab.
  
  \item [Packages]
  User can view the installed packages in this tab and load them manually.
  
  \item [Help]
  Documentation can be searched and viewed in this tab. Users can open documentation of a particular function using \verb|?| character in the interpreter. For instance, \verb|?sum| will open up the documentation for the \verb|sum()| function. 
  
\end{description}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{img/rstudio.PNG}
\caption{RStudio Server Pro (RSP)}
\end{figure}

\section{Packages}

Functionality in \verb|R| can be extended through packages. Most packages are open-sourced with a few exceptions. They are published on community-maintained repositories such as CRAN and Bioconductor.

The function \verb|install.packages()| can be used to install new packages from CRAN (or a CRAN-like source).

<<ex:installpackage, fig.show='hide', results='hide', eval=FALSE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize',rexample=TRUE>>=
# Return all installed packages
installed.packages()
# Install a new package and all its dependencies from CRAN
# This will install to the default library location
install.packages("ggplot2", dependencies = TRUE)
# Load an installed package 
# (both lines are identical)
library(ggplot2)
library("ggplot2")
@

The number of packages on CRAN has grown exponentially in the past few years. To help users select relevant packages for certain statistical topics, users can refer to the CRAN Task View\footnote{\url{https://cran.r-project.org/web/views/}}. It provides a curated list of packages which usually serves as good topical guide.

\chapter{Programming Concepts}


In \verb|R|, everything begins with variables. User can assign value to variable using the \verb|<-| symbol. The \verb|#| character is used for commenting. 

To name a variable, it is very common to name variables in \verb|R| using the 'camel case' convention. Sometimes 'snake case' and 'dot case' are also used. When create a new variable, it is important to avoid using reserved words. Reserved words can be checked by running the command \verb|?Reserved|.

<<ex:assignvariable, fig.show='hide', results='hide', eval=FALSE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize',rexample=TRUE>>=
# Assign variables
myVarX <- 5
myVarY <- 20
# Perform multiplication
myVarX * myVarY
# Look at the reserved words
?Reserved
@

\section{Vector}

\verb|R| is a vectorised programming language. Vector of consecutive integers can be created using the \verb|:| character. Alternatively, vector of discrete values can be created using the function \verb|c()|. All elements of a vector must have the same data type.

<<ex:createvectors, fig.show='hide', results='hide', eval=FALSE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize',rexample=TRUE>>=
# Create a vector of integers one to ten
myVec1 <- 1:10
# Find out the length of vector
length(myVec1)
# Reverse the vector
# This does not change the value of myVec1
rev(myVec1)
# Create a custom vector of 10, 15, 20, 25, 30
myVec2 <- c(10, 15, 20, 25, 30)
# Create a vector of sequential numbers with increment 0.5
myVec3 <- seq(from = -2, to = 2, by = 0.5)
# Elements of a vector can be named
myVec4 <- c(`New York` = 8.5,
            `London` = 8.6,
            `Moscow` = 11.9)
@

Users can select elements of a vector by subsetting using index number. In \verb|R|, index number starts from \(1,2,3,...\). For example, \verb|myVec2[2]| will return the second element of the vector.

<<ex:subsetting, fig.show='hide', results='hide', eval=FALSE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize',rexample=TRUE>>=
# Select the second element of the vector
myVec2[2]
# Subset a range from the vector
myVec2[2:4]
# Subset specified elements
myVec2[c(4,2,3)]
# Subset named element of a vector
myVec4["New York"]
@

Most operations in R are vectorised. Vectorisation means the operation is applied to the whole vector instead of individual elements. 
<<ex:arithmetic, fig.show='hide', results='hide', eval=FALSE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize',rexample=TRUE>>=
# Arithmetic operations
myVec1 + 10
myVec1 - 10
myVec1 * 2
myVec1 / 2
myVec1 ^ 2
log(myVec1)
@

\section{Data Types}

Users can create vectors of other data types easily in \verb|R|. Example \ref{ex:arithmetic} shows how to create character vector, date vector and \verb|POSIXct|(date/time) vector.

<<ex:datatypevector, fig.show='hide', results='hide', eval=FALSE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize',rexample=TRUE>>=
# Vector can contain character objects
myVec4 <- c("Bill", "Mark", "Steve", "Jeff", "Larry")
# Constant character vectors in R
LETTERS
letters
month.name
month.abb
# This is a vector of Date objects
myVec5 <- as.Date(c("2017-07-13",
                    "2017-10-11",
                    "2017-11-21",
                    "2018-01-16", 
                    "2018-03-27"))
# Load the lubridate package
# Use the function ymd_hms() to parse date/time with timezone
# Returns a vector of POSIXct (date/time) object
library(lubridate)
myVec6 <- ymd_hms(c("2017-07-13 09:30:00",
                    "2017-10-11 08:00:00",
                    "2017-11-21 10:00:00",
                    "2018-01-16 11:30:00", 
                    "2018-03-27 12:00:00"),
                  tz = "Europe/London")
# Date/time manipulation applied to a vector
myVec7 <- myVec6 + hours(1) + minutes(30)
# Compute the day of week - returns a vector of characters
weekdays(myVec7)
@

\section{Logical operators}

Logical operators can be applied on vector objects. It always returns a vector of logical values with the same length of the input vector. Examples \ref{ex:logical} 

<<ex:logical, fig.show='hide', results='hide', eval=FALSE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize',rexample=TRUE>>=
# Find all values greater than 5 - returns a vector of logical values 
myVec1 > 5
# Find all values equal to 7
myVec1 == 7
# Find all values matching 2,4,6 and 8
myVec1 %in% c(2,4,6,8)
# Find all values between 2 and 7
myVec1 >= 2 & myVec1 <= 7
# Find all values equal to 7 or equal to 8
myVec1 == 7 | myVec1 == 8
@

\section{Special Numbers}

There are special numbers in \verb|R|, which are shown in Example \ref{ex:specialnumbers}. For instance, the variable \verb|pi| is a constant \(3.14159...\). In most cases, missing values are usually indicated as \verb|NA| (Not Available). All operations involving \verb|NA| input always produce \verb|NA| output.

On the other hand, values returned by computational error are \verb|NaN| (Not-a-Number). Whereas mathematical infinities are indicated by \verb|Inf| and \verb|-Inf| respectively.

<<ex:specialnumbers, fig.show='hide', results='hide', eval=FALSE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize',rexample=TRUE>>=
# Pi is constant 3.14159...
pi
# One divided by zero is infinity
1/0
# Negative number divided by zero is negative infinity
-1/0
# Infinity divided by infinity is Not-a-Number (NaN)
Inf/Inf
# Not available (NA) plus one is still NA
NA + 1
# Effects of different special numbers
c(5, 10, 15, NA, 25, 30, NaN, 35, 40, Inf, 50, -Inf, 60) / 5
@

\section{List}

Objects with different data types can be held together by a \verb|list|. Example \ref{ex:list} illustrates a list containing objects of several data types. Elements of a \verb|list| can be optionally named for easier subsetting.

<<ex:list, fig.show='hide', results='hide', eval=FALSE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize',rexample=TRUE>>=
myFavBook <- list(`title` = "R for Data Science",
                  `authors` = c("Garrett Grolemund", "Hadley Wickham"),
                  `publishDate` = as.Date("2016-12-12"),
                  `price` = 18.17,
                  `currency` = "USD",
                  `edition` = 1,
                  `isbn` = 1491910399)
# Select a named element of a list
# Use the dollar sign, followed by name without bracket
myFavBook$title
# Use double squared brackets with element's name as character
myFavBook[["authors"]]
# Select the fourth element in the list
myFavBook[[4]]
@

\section{Data Frame and Tibble}

A data frame is a list of variables of the same number of rows with unique row names. In many cases, dataset extracted from CSV file or SQL server are returned as a data frame object. Example \ref{ex:dataframe} demonstrates how to construct a data frame.

<<ex:dataframe, fig.show='hide', results='hide', eval=FALSE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize',rexample=TRUE>>=
myFavMovies1 <- data.frame(`title` = c("Dr. No",
                                       "Goldfinger",
                                       "Diamonds are Forever",
                                       "Moonraker",
                                       "The Living Daylights",
                                       "GoldenEye",
                                       "Casino Royale"),
                           `year` = c(1962, 1964, 1971, 1979, 
                                      1987, 1995, 2006),
                           `box` = c(59.5, 125, 120, 210.3,
                                     191.2, 355, 599),
                           `bondActor` = c("Sean Connery",
                                           "Sean Connery",
                                           "Sean Connery",
                                           "Roger Moore",
                                           "Timothy Dalton",
                                           "Pierce Brosnan",
                                           "Daniel Craig"))
@

In modern \verb|R|, \verb|tibble| is the enhanced version of traditional data frame. More functions are available for \verb|tibble| objects. Example \ref{ex:tibble} shows how to construct a \verb|tibble| and append an extra row at the end.

<<ex:tibble, fig.show='hide', results='hide', eval=FALSE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize',rexample=TRUE>>=
library(tibble)
myFavMovies2 <- tibble(`title` = c("Dr. No",
                                   "Goldfinger",
                                   "Diamonds are Forever",
                                   "Moonraker",
                                   "The Living Daylights",
                                   "GoldenEye",
                                   "Casino Royale"),
                       `year` = c(1962, 1964, 1971, 1979, 
                                  1987, 1995, 2006),
                       `box` = c(59.5, 125, 120, 210.3,
                                 191.2, 355, 599),
                       `bondActor` = c("Sean Connery",
                                       "Sean Connery",
                                       "Sean Connery",
                                       "Roger Moore",
                                       "Timothy Dalton",
                                       "Pierce Brosnan",
                                       "Daniel Craig"))
# Append an extra row at the end of the tibble
# Rewrite the original tibble object
myFavMovies2 <- add_row(myFavMovies2, 
        title = "Spectre", year = 2015, box = 880.7,
        bondActor = "Daniel Craig")
@

Example \ref{ex:subsettibble} shows that a \verb|tibble| can be subsetted in various ways. The most common operation is selecting a column by name. 

<<ex:subsettibble, fig.show='hide', results='hide', eval=FALSE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize',rexample=TRUE>>=
# Get one column by name
myFavMovies2[["title"]]
myFavMovies2$title
# Get a range of columns by position ID
myFavMovies2[, 1:2]
myFavMovies2[1:2]
# Get rows 1 to 3
myFavMovies2[1:3, ]
# Get the "year" variable of row 1-3 
myFavMovies2[1:3, "year"]
# Get the "title" and "year" variables of row 4-7 
myFavMovies2[4:7, c("title","year")]
@

\section{Function}

User can create custom functions in \verb|R|. In example \ref{ex:function}, a new function \verb|is.odd()| is created. Result can be explicitly returned using \verb|return()|. Alternatively, the value of the function's last line is implicitly returned.

<<ex:function, fig.show='hide', results='hide', eval=FALSE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize',rexample=TRUE>>=
# Defines a custom function
is.odd <- function(x) {
  # The modulo operator %% returns the remainder
  # If a number divide by 2 gives remainder 1, then it is an odd number
  remainder <- x %% 2
  equalToOne <- remainder == 1
  return(equalToOne)
}
# Execute the function with one input
is.odd(5)
# Execute the function with an integer vector
is.odd(1:10)
# Define another function
is.even <- function(x) {
  !is.odd(x)
}
# Return true for even numbers
is.even(1:10)
@

\section{Flow Control}

\subsection{If-Else}

An \verb|if-else| statement is controlled by the condition. The \verb|if| part executes if the contition is \verb|TRUE|. Alternatively, the \verb|else| part will be executed. Example \ref{ex:ifelse} shows a simple \verb|if-else| control statement.

<<ex:ifelse, fig.show='hide', results='hide', eval=FALSE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize',rexample=TRUE>>=
# Loads the lubridate package for additional date/time functions
library(lubridate)
# Find out what day is today
myWeekday <- weekdays(today())
# Check whether today is Saturday or Sunday
if (myWeekday %in% c("Saturday","Sunday")) {
  myGreeting <- "Have a nice weekend"
} else {
  myGreeting <- "Go back to work"
}
# Prints the message
myGreeting
@

The statement can be further extended to consider multiple conditions. It checks the conditions sequentially and returns once a condition is met. This is shown in example \ref{ex:ifelse2}.

<<ex:ifelse2, fig.show='hide', results='hide', eval=FALSE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize',rexample=TRUE>>=
library(lubridate)
myWeekday <- weekdays(today())
# Checks multiple conditions
if (myWeekday %in% c("Saturday","Sunday")) {
  myGreeting <- "Have a nice weekend"
} else if(myWeekday == "Friday"){
  myGreeting <- "It's Friday!"
} else if(myWeekday == "Monday"){
  myGreeting <- "Oh no..."
} else {
  myGreeting <- "Go back to work"
}
myGreeting
@

\subsection{While}

The \verb|while| statement loops as long as the condition stays \verb|TRUE|. Example \ref{ex:while} demonstrates how \verb|while| loop can be implemented.

<<ex:while, fig.show='hide', results='hide', eval=FALSE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize',rexample=TRUE>>=
myCounter <- 100
while (myCounter > 0) {
  myCounter <- myCounter - 5
  print(myCounter)
}
@

Example \ref{ex:while2} shows that early exit can be implemented in a \verb|while| statement using the \verb|break| operator. The \verb|next| operator can be used to skip one of the iterations.

<<ex:while2, fig.show='hide', results='hide', eval=FALSE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize',rexample=TRUE>>=
myCounter <- 100
while (myCounter > 0) {
  myCounter <- myCounter - 5
  if (myCounter > 50) {
    # Skips all iterations if the counter value is greater than 50
    next
  }
  if (myCounter == 10) {
    # Early stop if the counter value matches 10
    break
  }
  print(myCounter)
}
@

\subsection{For}

\verb|For| loops can be used to execute certain operations again and again. In example \ref{ex:for}, the \verb|for| loop calculates the sum expression \( \sum_{i=1}^{100} i^2 \) by running the code sequentially many times.

<<ex:for, fig.show='hide', results='hide', eval=FALSE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize',rexample=TRUE>>=
myResult <- 0
for (i in 1:100) {
  myResult <- myResult + i ^ 2
}
myResult
@

\section{Apply}

Functions in the apply family can be used to run operations over multiple values. In example \ref{ex:apply}, the \verb|apply()| function receives a \verb|tibble| object and iterates over each row. Variables of each row is merged as a character message and returned. 

<<ex:apply, fig.show='hide', results='hide', eval=FALSE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize',rexample=TRUE>>=
# The second argument 1 inidicates iterate over rows
myMessages1 <- apply(myFavMovies2, 1, function(row){ 
  sprintf("%s was released in %s.", row["title"], row["year"])
})
myMessages1
@

There are several useful functions in the apply family. In example \ref{ex:lapply}, the \verb|lapply()| function receives an input and returns a \verb|list| as output.

<<ex:lapply, fig.show='hide', results='hide', eval=FALSE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize',rexample=TRUE>>=
myMessages2 <- lapply(myFavMovies2$title, 
                      function(x){ sprintf("%s is a great movie!", x) })
# Checks the data type of the result
typeof(myMessages2)
# Check whether it is a list
is.list(myMessages2)
# Select the 6th element of the list
myMessages2[[6]]
@

The \verb|sapply()| function works in a similar way. It returns a vector as the result. Example \ref{ex:sapply} shows how it can be used.

<<ex:sapply, fig.show='hide', results='hide', eval=FALSE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize',rexample=TRUE>>=
myMessages3 <- sapply(myFavMovies2$title, 
                      function(x){ sprintf("%s is a great movie!", x) })
# Check whether it is a list
is.list(myMessages3)
myMessages3
@

There are other useful members in the apply family, such as \verb|vapply()|,  \verb|tapply()|, \verb|mapply()|, \verb|rapply()| and \verb|eapply()|. Table \ref{tab:applyfunctions} shows a brief summary of the functions.

\begin {table}[H]
\caption {Functions in the apply family}
\begin{center}
\begin{tabular}{ | l | p{10cm} |}
\toprule[1.5pt]
{\bf Function}        & {\bf Brief Description}\\
\midrule
\verb|apply()|        & Apply function over a rows or columns of data frame. \\
\verb|lapply()|       & Apply function over a vector and returns a list. \\
\verb|sapply()|       & Apply function over a vector and returns a vector. \\
\verb|vapply()|       & Apply function over a vector and returns a fixed result.\\
\verb|tapply()|       & Apply function over a vector by groups.\\
\verb|mapply()|       & Multivariate version of \verb|sapply()|.\\
\verb|rapply()|       & Recursive version of \verb|lapply()|.\\
\verb|eapply()|       & Apply function over named values in an \verb|environment|.\\
\bottomrule[1.5pt]
\end{tabular}
\end{center}
\label{tab:applyfunctions}
\end{table}

\section{Efficient Looping}

Vectorisation can speed up code very significantly, therefore it is always a good idea to vectorise code for maximum performance.

In \verb|R|, looping can sometimes be slow. This is because loops and standard \verb|apply| functions are executed in serial. Parallelism can be used to solve this problem.

Moreover, \verb|R| objects are stored in memory. If the operation in the loop is altering the size of object, it will force \verb|R| to reallocate the object to a new memory address. For this reason, avoid modifying object size in loops can always speed up the code.

In example \ref{ex:looping}, the \verb|system.time()| function is used to capture the total time used to execute the code. The curly brackets \verb|{}| wraps multi-line code as one single expression. 

<<ex:looping, fig.show='hide', results='hide', eval=FALSE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize',rexample=TRUE>>=
# Vectorised operation is fast
system.time({
  myResult <- 1:100000 * 2
})
# Looping is quite slow
system.time({
  myResult <- sapply(1:100000, function(x){ x * 2 })
})
# Appending to vector is much slower
system.time({
  myResult <- c()
  for(i in 1:100000){
    myResult <- c(myResult, i * 2)
  }
})
@


\chapter{Data Transformation}
\label{ch:datatransformation}

The \verb|tidyverse| is a coherent system of packages for data manipulation, exploration and visualisation that share a common design philosophy. These were mostly developed by the prolific R developer Hadley Wickham\footnote{\url{http://hadley.nz}}, but they are now being expanded by several contributors. The \verb|tidyverse| packages are intended to make data scientists and statisticians more productive by guiding them through the workflow of a typical project. This is illustrated in figure \ref{tidyverseworkflow}.

You may being analysis by importing data into R. This typically means that you take data stored in a file, database, or web API and load it into R as an object known as a \verb|data frame| or a \verb|tibble|.

Once the data is imported, it is a good idea to tidy it. In brief, when your data is tidy, each column is a variable and each row is an observation. This is important because a consistent structure lets you focus your struggle on questions about the data, not fighting to get the data into the right form for different functions.

Once you have tidy data, the usual next step is to transform it. This includes narrowing in on observations of interest, creating new variables that are functions of existing variables and calculating summary statistics.

There are two main engines of knowledge generation: \begin{inparaenum}[1)] \item visualisation and \item modelling \end{inparaenum}. These have complementary strengths and weaknesses so a good analysis will iterate between them many times.

Data visualisation is a fundamentally human activity. A good visualisation will show you things that you did not expect, or raise new questions about the data. It might also cast doubt on the research hypothesis or hint that you need to collect different data. Visualisation does not scale particularly well because it requires human interpretation.

Models are complementary tools to visualisation. Once the research hypotheses are sufficiently precise, you can use a model to address them. Models are fundamentally mathematical or computational tools, so they generally scale well. 

The last step of data science is communication, which is a critical part of any data analysis project. It does not matter how well your models and visualisation have led you to understand the data unless you can also communicate your results to others.

Surrounding all these tools is programming. This is a tool that you use in every part of the analysis.

\begin{figure}[H]
	\centering
    \begin{tikzpicture}[node distance = 2.5cm, auto]
        \node [block3, dashed, label={above left:\tiny Program}] (bigbox){
            \begin{tikzpicture}[node distance = 2.5cm, auto, solid]
              \node [block1] (import) {Import};
              \node [block1, right of=import] (tidy) {Tidy};
              \node [block3, dashed, fill=gray!10, below of=tidy, label={above left:\tiny Understand}] (mybox){
                  \begin{tikzpicture}[node distance = 2.5cm, auto, solid]
                      \node [block2] (transform) {Transform};
                      \node [block2, right of=transform] (visualise) {Visualise};
                      \node [block2, below of=visualise] (model) {Model};
                      \draw [arrow] (transform) -- (visualise);
                      \draw [arrow] (visualise) -- (model);
                      \draw [arrow] (model) -| (transform);
                      \draw [arrow] (visualise) -- (transform);
                      \draw [arrow] (model) -- (visualise);
                      \draw [arrow] (transform) |- (model);
                  \end{tikzpicture}
              };
              \node [block1, below of=mybox] (communicate) {Communicate};
              \draw [arrow] (import) -- (tidy);
              \draw [arrow] (tidy) -- (mybox);
              \draw [arrow] (mybox) -- (communicate);
            \end{tikzpicture}
        };
    \end{tikzpicture}
  \label{tidyverseworkflow}
	\caption{Stages of data analysis}
\end{figure}

This list provides an overview of the main \verb|tidyverse| packages and how they fit into this typical workflow.

\begin{description}
  \item[Import] Reading datasets from various data sources.
    \begin{itemize}
      \item \verb|readr|
      \item \verb|readxl| 
      \item \verb|haven| 
      \item \verb|httr| 
      \item \verb|rvest|
      \item \verb|xml2|
    \end{itemize}
  \item[Tidy] Clean up datasets.
    \begin{itemize}
      \item \verb|tibble|
      \item \verb|tidyr|
    \end{itemize}
  \item[Transform] Aggregate, change variable format and derive new variables.
    \begin{itemize}
      \item \verb|dplyr|
      \item \verb|forcats| 
      \item \verb|hms| 
      \item \verb|lubridate| 
      \item \verb|stringr|
    \end{itemize}
  \item[Visualise] Creating charts using the Grammar of Graphics.
    \begin{itemize}
      \item \verb|ggplot2|
    \end{itemize}
  \item[Model] Train and test statistical models.
    \begin{itemize}
      \item \verb|broom|
      \item \verb|modelr|
    \end{itemize}
  \item[Program] Coding in pipeline-style.
    \begin{itemize}
      \item \verb|magrittr|
      \item \verb|purrr|
    \end{itemize}
\end{description}

In most cases, data does not come to you in exactly the right format. Often you need to compute new variables, or to summarise and rename the original ones. In some cases, you may have to reorder the observations to make the data easier to work with. An excellent package for these tasks is the \verb|dplyr| package. There are five key functions in this package that allow you to solve the vast majority of data manipulation challenges.

\begin{itemize}
\item Subset the observations by criteria - \verb|filter()|
\item Reorder the observations - \verb|arrange()|
\item Pick variables by name - \verb|select()|
\item Compute new variables as a function of existing variables - \verb|mutate()|
\item Collapse many values down to a single summary value - \verb|summarise()| or \verb|summarize()|
\end{itemize}

All aforementioned functions work similarly. The first argument is a \verb|data frame| containing the source data. The subsequent arguments describe what to do with the data. All the functions return the processed data in a new \verb|data frame|. These can all be used in conjunction with the \verb|group_by()| function, which changes the scope of each function from operating on the entire dataset to operating on it group-by-group. 

\section{Filtering}

You can use the function \verb|filter()| to subset observations based on their values. The first argument is a \verb|data frame|, while the subsequent arguments are the filtering expressions.

For filtering you can use the standard comparison operators (\verb|>|, \verb|>=|, \verb|<|, \verb|<=|, \verb|!=| (not equal), and \verb|==| (equal)). To combine multiple filtering arguments, you can separate them with a comma or use the \verb|&| symbol. In addition, logical operators can be used for more complex combinations.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{img/venn.PNG}
\caption{Logical operators in R}
\end{figure}

Missing values are represented in R as \verb|NA|. They are contagious because any operation involving an unknown value will produce unknown results. 

The function \verb|filter()| only includes rows where the condition is \verb|TRUE|. It excludes both \verb|FALSE| condition and \verb|NA| values.

You can use the function \verb|is.na()| to determine whether a value is missing. You can also use this to explicitly include missing values in a \verb|filter()| call.

In example \ref{ex:tidyfilter}, We will learn how to use \verb|tidyverse| using a dataset on flights departing from New York City in 2013.

<<ex:tidyfilter, fig.show='hide', results='hide', eval=FALSE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize',rexample=TRUE>>=
# load the required packages
library(dplyr)
library(nycflights13)
# Inspect the data, noting the type of each variable
flights
# View the flights dataset interactively
View(flights)
# Select all flights on January 1st
# Assigning them to a new variable jan1
jan1 <- filter(flights, month == 1, day == 1)
jan1
# Select all flights from November or December 
filter(flights, month == 11 | month == 12)
# A useful shorthand for this problem is x %in% y
# It selects every row where x is one of the values in y
filter(flights, month %in% c(11, 12))
# Let’s select only the flights that weren’t delayed
# (on arrival or departure) by more than two hours:
filter(flights, arr_delay <= 120, dep_delay <= 120)
@

\begin{Exercise}[title={Filtering Observations}]

Find all flights in the \verb|flights| dataset that:
\begin{enumerate}
\item Had an arrival delay of two or more hours.
\item Flew to Houston (IAH or HOU).
\item Were operated by United (\verb|UA|), American (\verb|AA|) or Delta (\verb|DL|).
\item Departed in summer (July, August and September)
\item Arrived more than two hours late, but did not leave late.
\item Were delayed by at least an hour, but made up over \(30\) minutes in flight.
\item How many flights have a missing \verb|dep_time|? What other variables are missing?
\end{enumerate}

\end{Exercise}

\section{Sorting}


The function \verb|arrange()| changes the order of observations in a dataset. It sorts the observations by a specified set of variable names\footnote{More complicated expressions can also be used for ordering.} in ascending order. 

If you provide more than one variable name, each additional variable will be used to break ties in the values of preceding variable. Use \verb|desc()| to reorder variable in descending order. Missing values are always sorted at the end. Example \ref{ex:tidyarrange} shows how the \verb|arrange()| function can be used.

<<ex:tidyarrange, fig.show='hide', results='hide', eval=FALSE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize',rexample=TRUE>>=
# Arrange flights by year, then month, then day
arrange(flights, year, month, day)
# Use desc() to reorder by a column in descending order
arrange(flights, desc(arr_delay))
# Missing values are always sorted at the end:
df <- tibble(x = c(5, 2, NA))
arrange(df, x)
arrange(df,desc(x))
@

\begin{Exercise}[title={Arranging Observations}]
Sort the \verb|flights| dataset to find:
\begin{enumerate}
  \item The fastest flights.
  \item The most delayed flights.
  \item The flights that left earliest.
  \item Which flights travelled the longest?
\end{enumerate}

\end{Exercise}

\section{Subsetting Variables}

It is not uncommon to get datasets with hundreds or even thousands of variables. In this case, the first challenge is often narrowing in on the variables you are actually interested in.

The \verb|select()| function allows you to rapidly zoom in on a useful subset using operations based on the names of the variables.

There are a number of helper functions you can use within \verb|select()|. These are also shown in example \ref{ex:tidyselect}:
\begin{enumerate}
  \item \verb|starts_with("abc")| matches variable names that begin with \verb|"abc"|.
  \item \verb|ends_with("xyz")| matches variable names that end with \verb|"xyz"|.
  \item \verb|contains("ijk")| matches variable names that contain \verb|"ijk"|.
  \item \verb|num_range("x", 1:3)| matches \verb|x1|, \verb|x2|, and \verb|x3|.
\end{enumerate}

The function \verb|select()| can in principle be used to rename variables, but it drops all of the variables not explicitly mentioned. Therefore it is better to use the \verb|rename()| function which keeps all variables not explicitly mentioned.

Another option is to use \verb|select()| in conjunction with the \verb|everything()| helper. This is useful if you want to move several variables to the start of the \verb|data frame|. 

<<ex:tidyselect, fig.show='hide', results='hide', eval=FALSE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize',rexample=TRUE>>=
# Select columns by name
select(flights, year, month, day)
# Select all columns between ‘year’ and ‘day’ (inclusive)
select(flights, year:day)
# Select all columns except those from year to day (inclusive)
select(flights, -(year:day))
# Rename a variable using rename()
rename(flights, tail_num = talinum)
# Reorder columns using the everything() helper
select(flights, time_hour, air_time, everything())
@

\begin{Exercise}[title={Selecting Variables}]

\begin{enumerate}
  \item Can you find two ways to select \verb|dep_time|, \verb|dep_delay|, \verb|arr_time| and \verb|arr_delay| from the \verb|flights| dataset in one line of code?
  \item What happens if you include the name of a variable multiple times in a \verb|select()| call?
\end{enumerate}

\end{Exercise}

\section{Compute Variables}
In the data transformation process, it is often useful to compute new variables that are functions of existing ones. For this we can use the \verb|mutate()| function. This function always adds new variables at the end of the dataset. Alternatively, you can use the \verb|transmute()| function if you only want to keep the newly-computed variables and remove the old ones. For both \verb|mutate()| and \verb|transmute()|, you can refer to columns that you have just created in the same function call.

There are many useful creation functions you can use with \verb|mutate()| to create new variables:

\begin{description}
  \item [Arithmetic operators] The operators \verb|+|, \verb|-|, \verb|*|, \verb|/|, \verb|^| are all vectorised using the so-called 'recycling rules' (i.e. if one parameter is shorter than the other, it will be automatically extended to be the same length.)

  \item [Modular arithmetic] \verb|%/%| for integer division (discards remainder) and \verb|%%| for remainder only (modulo).

  \item [Logs] Very useful for dealing with data that ranges across multiple orders of magnitude.

  \item [Logical comparison] \verb|<|, \verb|<=|, \verb|>|, \verb|>=|, \verb|!=|

  \item[Ranking] There are several of these – the most common one is \verb|min_rank()| which does the most usual type of ranking (e.g. first, second, third, fourth) and gives the smallest values the smallest ranks.
\end{description}

In example \ref{ex:tidymutate}, we will start by creating a narrower dataset so we can see the new variables.

<<ex:tidymutate, fig.show='hide', results='hide', eval=FALSE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize',rexample=TRUE>>=
# Select several columns only
flights_sml <- select(flights,
                      year:day,
                      ends_with("delay"),
                      distance,
                      air_time)
# Use these the smaller data frame derive new columns
mutate(flights_sml,
  	gain = arr_delay - dep_delay,
  	speed = distance / air_time * 60)
# You can refer to columns that you have just created
mutate(flights_sml,
       gain = arr_delay - dep_delay,
       hours = air_time / 60,
       gain_per_hour = gain / hours)
# Keep only the new variables using transmute()
transmute(flights,
          gain = arr_delay - dep_delay,
          hours = air_time / 60,
          gain_per_hour = gain / hour)
# Modular arithmetic: modulo
7 %% 2
# Modular arithmetic: integer division
7 %/% 2
# Compute hour and minute from dep_time
transmute(flights,
          dep_time,
          hour = dep_time %/% 100,
          minute = dep_time %% 100)
# Example of ranking
y <- c(1, 2, 2, NA, 3, 4)
min_rank(y)
min_rank(desc(y))
@

\begin{Exercise}[title={Computing New Variables}]

\begin{enumerate}
  \item Currently \verb|dep_time| and \verb|sched_dep_time| are convenient to look at but hard to compute with because they’re not really continuous numbers. Convert them to a more convenient representation of number of minutes since midnight.
  \item Compare new variable \verb|air_time| as \verb|arr_time - dep_time|. What do you expect to see? What problem do you see? What do you need to do to fix it?
  \item What does \verb|1:3 + 1:10| return and why? How about \verb|1:3 + 1:9|?
\end{enumerate}

\end{Exercise}

\section{Summarising}


The last key function is \verb|summarise()| or \verb|summarize()|. It collapses a set of values into one. This function is particularly useful when used in conjunction with \verb|group_by()|. This changes the unit of analysis from the whole dataset to individual groups. Then you can use functions on the grouped \verb|data frame| in order to obtain grouped summaries. These summary functions can be used in the \verb|summarise()| function:

\begin{description}
\item [Measures of location] Arithmetic average \verb|mean()| and median \verb|median()|.
\item [Measures of spread] Standard deviation \verb|sd()| and interquartile range \verb|IQR()|.
\item [Measures of rank] Minimum value \verb|min()|, maximum value \verb|max()| as well as the quantiles \verb|quantile()|
\item [Measures of position] \verb|first()|  and \verb|last()|
\end{description}

Whenever doing any aggregation, it is always a good idea to include either the total count \verb|n()| or the count of non-missing values \verb|sum(!is.na(x))|. To count the number of unique values, use \verb|n_distinct()|. By including count statistics, you can make sure that you are not drawing conclusions based on small amount of data. 

When combining several operations it is usually better to join them together using the pipe operator \verb|%>%| rather than repeatedly making new variables. This is illustrated in example \ref{ex:tidysummarise}.

<<ex:tidysummarise, fig.show='hide', results='hide', eval=FALSE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize',rexample=TRUE>>=
# Example of summarise() function alone
# You can also use summarize()
# These two are equivalent
summarise(flights, delay = mean(dep_delay, na.rm = TRUE))
# Combining summarise() with group_by() and a dplyr verb
by_day <- group_by(flights, year, month, day)
summarise(by_day, delay = mean(dep_delay, na.rm = TRUE))
# Multiple operations without a pipe
by_dest <- group_by(flights, dest)
delay <- summarise(by_dest,
                   count = n(),
                   dist = mean(distance, na.rm = TRUE),
                   delay = mean(arr_delay, na.rm = TRUE))
filter(delay, count > 20)
# There are three steps to prepare the flight delay data: 
# 1) Group flights by destination
# 2) Summarise average distance, delay and number of flights
# 3) Filter to remove noisy points
# They can be chained together using pipeline '%>%'
mySummary <- flights %>%
  group_by(dest) %>%
  summarise(
    count = n(),
    dist = mean(distance, na.rm = TRUE),
    delay = mean(arr_delay, na.rm = TRUE)) %>%
  filter(count > 20)
# Using summarise() to measure standard deviation
# Distance to some destination has larger spread than others
flights %>%
 	group_by(dest) %>% 
 	summarise(dist_sd = sd(distance, na.rm = TRUE)) %>%
 	arrange(desc(dist_sd))
@

\begin{Exercise}[title={Grouped Summaries}]

\begin{enumerate}
  \item Which carrier has the worst delays?
  \item For each destination, compute the total minutes of delay. For each flight, compute the proportion of the total delay for its destination.
\end{enumerate}

\end{Exercise}


\chapter{Regression Models}

Regression model is one of the most widely used statistical techniques in the realm of supervised learning. It quantifies the relationship between dependent variable \(Y\) and independent variable \(X\), where \(Y	\subseteq \mathbb{R} \) and \(X = \{ x_1, x_2,...x_M \}\).

\section{Linear Regression}

Linear regression\footnote{Also called ordinary least square (OLS) regression.} is a very popular parametric statistical technique. It can quantify the effects of each input variable, it also informs users whether the effects are statistically significant or not.

In a univariate scenario where \(x\) is the only input, it provides the best fit for the model equation \(\hat{y}_i=\beta_0+\beta_1 x_i \). The parameter \(\beta_0\) is normally referred as the intercept, while the \(\beta_1\) value is called the slope.

Simple linear model can be extended to include a high-order polynomial term  of variable \(x\). It provides higher model flexibility so that linear model fits the data better. For example, an \(M\)\textsuperscript{th} order polynomial term has been fitted to the dataset on the next chart. The choice of \(M\) is subjective but usually a small value of \(M\) is desirable because it helps avoid overfitting.

The model assumes model residuals \( \epsilon_i = y_i-\hat{y}_i\) are drawn from Gaussian distribution (i.e. having a bell-shaped curve). The goal of linear regression is to minimise the sum of squared residual. 

In the R language you can call the function \verb|lm()| to perform linear regression. It requires at least two arguments (\verb|data| and \verb|formula|). For example, \verb|lm(y ~ x , myData)| will perform a simple univariate linear model using independent variable \(x\) to predict dependent variable \(y\) in the data frame object \verb|myData|.

Dummy variable can be easily created from categorical labels. Users can simply use the syntax \verb|lm(y ~ x1 + x2, myData)| where \verb|x1| is a numeric variable and \verb|x2| either a factor or a character variable The \verb|lm| function will automatically create dummy variables on-the-fly. Normally the first category in the column will be used as reference level.

Simple linear model are often not flexible enough to model complex variable effects. In light of this, linear models can be made more flexible by including polynomial terms. It can be expressed as \verb|lm(y~poly(x1,3)+x2,myData)|. In this case, we are defining a cubic relationship with variable \verb|x1| and a linear relationship with variable \verb|x2| (Five coefficients in total will be estimated, four from the regression terms plus one from the intercept). Such model can be expressed as equation \eqref{linearexample}. 

\begin{equation}
\label{linearexample}
\hat{y}_i = \underbrace{\beta_0}_\text{Intercept} + 
\underbrace{\beta_1 x_{1,i} + \beta_2 x_{1,i}^2 +\beta_3 x_{1,i}^3}_\text{Cubic polynomial term} + 
\underbrace{\beta_4 x_{2,i}}_\text{Linear term}
\end{equation}

Interaction refers to the combined effect\footnote{Also known as synergy effect.} of more than one independent variables. For example, independent variable \verb|x1| and \verb|x2| might have no effect on dependent variable \(y\) alone. However, the effect on \(y\) can become prominent when these two variables are combined. In R language you can use the syntax \verb|lm(y ~ x1*x2)| to represent the relationship. The function \verb|I(x1*x2)| can be used to supress interaction term and the arguments will be treated as simple arithmetic operations.

\begin{Exercise}[title={Simple Linear Regression}]
In this exercise, we are going to predict car efficiency using the \verb|mtcars| teaching dataset. The dataset is embedded in open source R and can be called directly by \verb|mtcars|. In example \ref{ex:mtcars_head},you can peek at the dataset by executing \verb|head(mtcars)| or \verb|tail(mtcars)|. You can also read the dataset definition by executing the command \verb|?mtcars|.

<<ex:mtcars_head, eval=FALSE, size='scriptsize',rexample=TRUE>>=
#Load the dataset into your local environment.
data(mtcars)
# Browse the top few rows
head(mtcars)
# Browse the last few rows
tail(mtcars)
@

Before running any models, we can explore the dataset a bit further through visualisation. The R package \verb|ggplot2| is a very popular visualisation add-in. It builds charts by stacking layers of graphics on it. Example \ref{ex:mtcars_ggplot} shows how you can use \verb|ggplot2| to visualise the dataset.

<<ex:mtcars_ggplot, fig.show='hide', eval=FALSE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize',rexample=TRUE>>=
# Create a simple histogram using base R plot
hist(mtcars$mpg)
# Using dplyr pipeline style code (equivalent output)
library(dplyr)
mtcars$mpg %>% hist()
# Plot histogram using ggplot2 package
library(ggplot2)
mtcars %>%
  ggplot(aes(x=mpg)) + 
  geom_histogram() + 
  labs(x="Miles-per-gallon",
       y="Count", 
       title="Histogram showing the distribution of car performance")
# Scatterplot showing the relationship between mpg and wt
library(ggplot2)
mtcars %>%
  ggplot(aes(x=wt, y=mpg, colour=factor(cyl))) +
  geom_point() +
  labs(x="Weight",
       y="Miles-per-gallon",
       colour="Number of cylinders",
       title="Scatterplot showing car weight against performance")
# Create a boxplot showing mpg distribution of different gear types
mtcars %>%
  ggplot(aes(x=factor(am,
                    levels = c(0,1),
                    labels = c("Automatic", "Manual")), y=mpg)) + 
  geom_boxplot() +
  labs(x="Gear type",
       y="Miles-per-gallon")
# Draw a scatterplot with facets to visualise multiple variables 
mtcars %>%
  ggplot(aes(x=hp, y=mpg, colour=factor(gear), size=disp)) +
  geom_point() +
  facet_grid(. ~ cyl) +
  labs(x="Horsepower",
       y="Miles-per-gallon",
       colour="Number of gears",
       size="Displacement")
# Draw a matrix scatterplot
pairs(mtcars)
# Load the package GGally
# It is an extension of the ggplot2 package
# Use ggpairs to draws a prettier matrix scatterplot
library(GGally)
ggpairs(mtcars)
@

Now let us try to analyse car efficiency using the \verb|mpg| column as dependent variable. The hypothesis is that heavier cars have lower miles-per-gallon. We can investigate this by building a univariate linear model using the \verb|lm()| function and analyse the results. The following code fits an univariate regression model. The function \verb|summary(myModel1)| would print out all the key results of the model. This is demonstrated in example \ref{ex:linearmodel}.

<<ex:linearmodel, fig.show='hide', eval=TRUE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize',rexample=TRUE>>=
# Build a univariate linear model
myModel1 <- lm(mpg ~ wt, mtcars)
# Read the model summary
summary(myModel1)
@

The model summary contains a lot of useful information. Table \ref{tab:linearsummary} lists the key items alongside a short description.

\begin {table}[H]
\caption {Summary of key model information}
\begin{center}
\begin{tabular}{ | l | p{8cm} |}
\toprule[1.5pt]
{\bf Term}          & {\bf Description}\\
\midrule
Residuals	          & This is the unexplained bit of the model, defined as observed value minus fitted value (\(\epsilon=y_i-\hat{y_i}\)). If the model's parametric assumption is correct, the mean and median values of the residuals should be very close to zero. The distribution of the residuals should have equal tails on both ends.\\
Estimate            & Coefficient of the corresponding independent variable (i.e.  the \(\beta_m\) values).\\
Standard error      & Standard deviation of the estimate.\\
\(t\)-value         & The number of standard deviations away from zero (i.e. the null hypothesis).\\
\(P(>|t|)\)        & \(p\)-value of the model estimate. In general, variables with \(p\)-value above \(0.05\) are considered statistical significant.\\
Multiple \(R^2\)    & Pearson's correlation squared which indicates strength of relationship between the observed and fitted values.\\
Adjusted \(R^2\)    & Adjusted version of \(R^2\).\\
\(F\)-statistic     & Global hypothesis for the model as a whole.\\
\bottomrule[1.5pt]
\end{tabular}
\end{center}
\label{tab:linearsummary}
\end{table}

You can also build more complex multivariate models using the same \verb|lm()| function Example \ref{ex:linearmodelpoly} shows how the function deals with nominal and ordinal variables. You can either force the variable to become categorical by explicitly state \verb|factor(myVar)| in the formula. Alternatively, if the variable already belongs \verb|factor| data type, the \verb|lm()| regression function would handle it automatically without having to state it in the formula.

<<ex:linearmodelpoly, fig.show='hide', eval=TRUE, results='hide', warning=FALSE, message=FALSE, error=FALSE, size='scriptsize',rexample=TRUE>>=
# Dummy variables are automatically created on-the-fly.
# The variable am has two categories 
myModel2 <- lm(mpg ~ wt + hp + qsec + factor(am), mtcars)
summary(myModel2)
# Build a multivariate linear model with polynomial terms.
# Interaction effect can be added as well
myModel3 <- lm(mpg ~ wt + qsec + factor(am) + 
                 factor(cyl) * disp + poly(hp, 3) + 
                 factor(gear), mtcars)
summary(myModel3)
@

To further analyse the effects of individual variables, wou can load the \verb|car| package and use the function \verb|avPlots()| to view the partial regression plot\footnote{It is also called the added-variable plot.}. This would graphically display the effect of individual variables while keeping others in control. This is shown in example \ref{ex:avplots}.

<<ex:avplots,fig.show='hide', eval=TRUE, results='hide', warning=FALSE, message=FALSE, error=FALSE, size='scriptsize', rexample=TRUE>>=
library(car)
avPlots(myModel2)
@

\begin{figure}[H]
	\centering
  <<fig.show='asis', eval=TRUE,echo=FALSE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize', fig.width=9, fig.height=8>>=
  par(mfrow = c(2,2))
  library(car)
  avPlots(myModel2)
  par(mfrow = c(1,1))
  @
	\caption{Partial Regression plots}
\end{figure}

As the input predictor variables usually have different scale, the model coefficient are not directly comparable. To fairly compare the effect magnitude of the predictor variables, they need to be standardised first. The \verb|QuantPsyc| package has a function \verb|lm.beta()| which performs coefficient standardisation. The magnitude of the standardised coefficient indicates the relative influence of each predictor variable. You can follow example \ref{ex:betanorm} for coefficient standardisation.

<<ex:betanorm, fig.show='hide', eval=TRUE, results='hide', warning=FALSE, message=FALSE, error=FALSE, size='scriptsize', rexample=TRUE>>=
library(QuantPsyc)
lm.beta(myModel1)
lm.beta(myModel2)
lm.beta(myModel3)
@

You can also compare nested models\footnote{Refers to models having additional predictor terms. For example, \(\hat{y}=x_1+x_2+x_3\) and \(\hat{y}=x_1+x_2+x_3+x_4\) are both nested models of \(\hat{y}=x_1+x_2\).} using ANOVA technique. Example \ref{ex:linearanova} compares three nested models by applying Chi-square test on the model residuals to check for statistically significant differences. In most cases, the simpler model is prefereable if the candidate models are not significantly different.

<<ex:linearanova, fig.show='hide', eval=FALSE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize', fig.width=4, fig.height=3, rexample=TRUE>>=
# Compare linear regression models using Chi-square test
# Testing whether myModel2 and myModel3 are different from myModel1
anova(myModel1, myModel2, myModel3, test="Chisq")
@

\end{Exercise}


\begin{Exercise}[title={Regression Diagnostics}]

Linear regression is a parametric statistical method which has strong underlying assumptions. Analysing the model's diagnostic measurements would help us assess whether the assumptions are sufficiently met. You may use the \verb|plot()| function to create a series of regression diagnostic plots. The command in example \ref{ex:diagplots} generates several diagnostic plots for a standard linear regression model object.

<<ex:diagplots, fig.show='hide', eval=FALSE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize', fig.width=4, fig.height=3, rexample=TRUE>>=
plot(myModel3)
@

\begin{figure}[H]
	\centering
  <<fig.show='asis', eval=TRUE,echo=FALSE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize', fig.width=9, fig.height=8>>=
  par(mfrow = c(2,2))
  plot(myModel3)
  par(mfrow = c(1,1))
  @
	\caption{Regression diagnostic plots}
\end{figure}

\begin{description}
  \item [Residuals vs Fitted]
  Checks for non-linear relationship. For a good linear fit, residuals are normally distributed along a straight line centred at zero (i.e. a horizontal line). Contrarily, curvy line indicate poor model fit with possible non-linear effects.

  \item [Normal Quantile-Quantile]
  One the of main assumptions of linear regression is that the residual is drawn from zero-centred Gaussian distribution \(\epsilon_i\sim\mathcal{N}(0,\sigma^2)\). To verify whether the proposed model satisfies this assumption, we can use a normal quantile-quantile plot (Q-Q plot) to perform a quick check. It aligns model residuals against a theoretical normal distribution. If the residuals spread along a straight line on the Q-Q plot, it suggests that the residuals are normally distributed. Alternatively, if the data points deviate from the line it indicates vice versa. In this case, parametric model assumption does not hold and you might have to consider improving your model.
		
  \item [Scale-Location]
  Shows the distribution of the standardised residuals along the range of fitted values. As standard linear model is assumed to be homoscedastic, the residual variance should not vary along the range of fitted values (i.e. expects a near-horizontal line). If the standardised residual forms a distinguishable patten (e.g. fanning out or curvy), then the model may be heteroscedastic and hence violates the underlying assumption.
  
  \item [Residual vs Leverage (Cook's Distance)]
  Observations having high leverage pose greater influence to the model. This means that the model estimates are strongly affected by these cases. If such obserations have high residuals (i.e. large Cook's distance), they can sometimes be considered as outliers. On the other hand, most observations would have low leverage and short Cook's distance. This means that the model estimates would not have varied a lot if few such observations were to be added or discarded.
\end{description}
\end{Exercise}


\begin{Exercise}[title={Model Overfitting}]

Linear regression can be made more flexible by increasing the order of the polynomial terms. This allows linear model to capture non-linear effects. However, a flexible model also risks overfitting the data. This means that the model might appear to have very good fit during training, but it may fit poorly when it is tested on new data. In general, overfitted models have very little inference and are ungeneralisable. The code snippet in example \ref{ex:overfitting} runs a linear model with variable level of flexibility. 

<<ex:overfitting, fig.show='hide', eval=FALSE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize', fig.width=4, fig.height=3, rexample=TRUE>>=
# Bivariate linear model with polynomial terms
# You can change the values here.
J <- 3
K <- 2
myModel4 <- lm(mpg ~ poly(wt,J) + poly(hp,K), mtcars)
summary(myModel4)
# Create the base axes as continuous values
wt_along <- seq(min(mtcars$wt), max(mtcars$wt), length.out = 50)
hp_along <- seq(min(mtcars$hp), max(mtcars$hp), length.out = 50)
# Use the outer product of wt and hp to run predictions for every 
# point on the plane
f <- function(k1, k2, model){ z <- predict(model, data.frame(wt=k1, hp=k2 )) }
myPrediction <- outer(wt_along, hp_along, f, model = myModel4)
# Draw the 3D plane
myPlane <- persp(x = wt_along, xlab = "Weight",
                 y = hp_along, ylab = "Horsepower",
                 z = myPrediction, zlab = "Miles-per-Gallon",
                 main = "Fitted vs observed values in 3D space",
                 theta = 30, phi = 30, expand = 0.5, col = "lightblue")
# Add original data as red dots on the plane
myPoints <- trans3d(x = mtcars$wt,
                    y = mtcars$hp,
                    z = mtcars$mpg,
                    pmat=myPlane)
points(myPoints, col="red")
@

\begin{figure}[H]
	\centering
	\[\hat{y} = \beta_0 + \sum\limits_{j=1}^{3} {\beta_{wt}}_j x_{wt}^j + \sum\limits_{k=1}^{2} {\beta_{hp}}_k x_{hp}^k \]
	<<fig.show='asis', echo=FALSE, eval=TRUE, warning=FALSE, message=FALSE, error=FALSE, fig.width=8, fig.height=4>>=
  J <- 3
  K <- 2
  myModel4 <- lm(mpg ~ poly(wt,J) + poly(hp,K), mtcars)
  wt_along <- seq(min(mtcars$wt), max(mtcars$wt), length.out = 50)
  hp_along <- seq(min(mtcars$hp), max(mtcars$hp), length.out = 50)
  f <- function(k1, k2, model){ z <- predict(model, data.frame(wt=k1, hp=k2 )) }
  myPrediction <- outer(wt_along, hp_along, f, model = myModel4)
  myPlane <- persp(x = wt_along, xlab = "Weight",
                   y = hp_along, ylab = "Horsepower",
                   z = myPrediction, zlab = "Miles-per-Gallon",
                   theta = 30, phi = 30, expand = 0.5, col = "lightblue")
  myPoints <- trans3d(x = mtcars$wt,
                      y = mtcars$hp,
                      z = mtcars$mpg,
                      pmat=myPlane)
  points(myPoints, col="red")
  @
	\caption{A less flexible model showing better generalisability}
\end{figure}

\begin{figure}[H]
	\centering\[\hat{y} = \beta_0 + \sum\limits_{j=1}^{8} {\beta_{wt}}_j x_{wt}^j + \sum\limits_{k=1}^{5} {\beta_{hp}}_k x_{hp}^k \]
  <<fig.show='asis', echo=FALSE, eval=TRUE, warning=FALSE, message=FALSE, error=FALSE, fig.width=8, fig.height=4>>=
  J <- 8
  K <- 5
  myModel4 <- lm(mpg ~ poly(wt,J) + poly(hp,K), mtcars)
  wt_along <- seq(min(mtcars$wt), max(mtcars$wt), length.out = 50)
  hp_along <- seq(min(mtcars$hp), max(mtcars$hp), length.out = 50)
  f <- function(k1, k2, model){ z <- predict(model, data.frame(wt=k1, hp=k2 )) }
  myPrediction <- outer(wt_along, hp_along, f, model = myModel4)
  myPlane <- persp(x = wt_along, xlab = "Weight",
                   y = hp_along, ylab = "Horsepower",
                   z = myPrediction, zlab = "Miles-per-Gallon",
                   theta = 30, phi = 30, expand = 0.5, col = "lightblue")
  myPoints <- trans3d(x = mtcars$wt,
                      y = mtcars$hp,
                      z = mtcars$mpg,
                      pmat=myPlane)
  points(myPoints, col="red")
  @
	\caption{A more flexible model illutrating the risk of overfitting}
\end{figure}

\end{Exercise}


\section{Poisson Regression}

In the previous section, simple linear regression model assumes the dependent variable \(y\) follows a Gaussian distribution which spans the range \((-\infty, +\infty)\). Yet sometimes we would like to estimate the number of discrete events where it is often a positive integer (\(\mathbb{N}=\{0,1,2,3,...\}\)). In this case, Poisson regression can be used. It is based on Poisson distribution\footnote{A Poisson distribution is defined by a single parameter \(y \sim Poisson(\lambda)\), where the mean \(\mu\) and variance \(\sigma^2\) are equal. i.e. \(\lambda=\mu\) and \(\lambda=\sigma^2\)} which can be found in everyday life, the following are typical examples:

\begin{itemize}
  \item Number of children in a household.
  \item Number of bank notes in a wallet.
\end{itemize}

Poisson regression model can take into account multiple predictor variables. Equation \eqref{poisson} shows a Poisson regression model with \(\hat{y}\) as dependent variable and \(M\) predictor variables.

\begin{equation}
\label{poisson}
\hat{y}=e^{\beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_M x_M}
\end{equation}

\begin{figure}[H]
\centering
<<fig.show='asis', echo=FALSE, eval=TRUE, warning=FALSE, message=FALSE, error=FALSE, fig.width=5, fig.height=2>>=
  library(dplyr)
  library(ggplot2)
poisson_max <- 20
tibble(density = sapply(seq(0,poisson_max,1), dpois, lambda=1), frequency=0:poisson_max, lambda=1) %>%
  union_all(tibble(density = sapply(seq(0,poisson_max,1), dpois, lambda=2), frequency=0:poisson_max, lambda=2)) %>%
  union_all(tibble(density = sapply(seq(0,poisson_max,1), dpois, lambda=3), frequency=0:poisson_max, lambda=3)) %>%
  union_all(tibble(density = sapply(seq(0,poisson_max,1), dpois, lambda=4), frequency=0:poisson_max, lambda=4)) %>%
  union_all(tibble(density = sapply(seq(0,poisson_max,1), dpois, lambda=5), frequency=0:poisson_max, lambda=5)) %>%
  union_all(tibble(density = sapply(seq(0,poisson_max,1), dpois, lambda=6), frequency=0:poisson_max, lambda=6)) %>%
  union_all(tibble(density = sapply(seq(0,poisson_max,1), dpois, lambda=7), frequency=0:poisson_max, lambda=7)) %>%
  union_all(tibble(density = sapply(seq(0,poisson_max,1), dpois, lambda=8), frequency=0:poisson_max, lambda=8)) %>%
  ggplot(aes(x=frequency, y=density, colour=factor(lambda))) +
  geom_line() + 
  labs(x="Frequency", y="Density", colour="Lambda") +
  scale_colour_brewer(palette="Set2") +
  theme_classic(base_family = "serif")
@
\caption{Poisson distribution with different \(\lambda\) values}
\end{figure}


\begin{Exercise}[title={Testing for Poisson Distribution}]

Using the \verb|mtcars| dataset, we can estimate the number of carburetors (i.e. the variable \verb|carb|) in different cars. In example \ref{ex:meanvar}, you can use the command \verb|hist(mtcars$carb)| to draw a simple histogram. You should find that this variable \begin{inparaenum}[1)]
\item never goes below zero and
\item has a long but thin tail towards the positive side
\end{inparaenum}. These are key signatures of a Poisson distribution.

<<ex:meanvar, fig.show='hide', eval=FALSE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize', fig.width=4, fig.height=3, rexample=TRUE>>=
# Draw a simple histogram.
hist(mtcars$carb)
# Compute the mean and variance.
mean(mtcars$carb)
var(mtcars$carb)
@

To robustly check whether a variable is truly drawn from a Poisson distribution, you can perform a Chi-squared goodness-of-fit test. It fits the input data against a theoretical Poisson distribution. Example \ref{ex:goodfit} visualises the results. The vertical bars would fill the positive space if the input data fits well against the Poisson distribution. This can be statistically examined by analysing the \(p\)-value of the Chi-square test. If the \(p\)-value is small enough, we can then accept the hypothesis that the variable is drawn from a Poisson distribution. 

<<ex:goodfit, fig.show='hide', eval=TRUE, results='hide', warning=FALSE, message=FALSE, error=FALSE, size='scriptsize', fig.width=8, fig.height=4, rexample=TRUE>>=
# Performs the Chi-squared goodness-of-fit test.
# It checks whether the variable is drawn from a Poisson distribution.
library(vcd)
gf <- goodfit(mtcars$carb, type= "poisson", method= "ML")
# Plots the observed frequency vs theoretical Poisson distribution.
# The hanging bars should fill the space if it is perfectly Poisson.
plot(gf)
# Checks the statistical p-value of the goodness-of-fit test.
# If p<=0.05 then it is safe to say that the variable is Poisson.
summary(gf)
@

\begin{figure}[H]
\centering
<<fig.show='asis', echo=FALSE, eval=TRUE, warning=FALSE, message=FALSE, error=FALSE, fig.width=8, fig.height=6>>=
plot(gf)
@
\caption{Goodness-of-fit test for Poisson distribution}
\end{figure}


\end{Exercise}

\begin{Exercise}[title={Building a Poisson Model}]

Example \ref{ex:poissonreg} trains a Poisson regression model using the \verb|mtcars| dataset. The variable \verb|carb| is used as dependent variable while \verb|hp|, \verb|wt| and \verb|am| are used as independent predictor variables. The regression output can be interpreted in a similar way as the ones from linear model.

<<ex:poissonreg, fig.show='hide', eval=FALSE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize', fig.width=4, fig.height=3, rexample=TRUE>>=
# Build a Poisson model to predict the number of carburetors in a car.
myPoissonModel <- glm(carb ~ hp + wt + factor(am), 
                      family="poisson",
                      data=mtcars)
# Read the model summary
summary(myPoissonModel)
# Read the model diagnostic
plot(myPoissonModel)
# Visualise the observed / fitted values as a table
tibble(observed = mtcars$carb,
       fitted = myPoissonModel$fitted.values) %>% View()
@
\end{Exercise}

\section{Logistic Regression}

Logistic regression can be used if the dependent variable is binary. This refers to when the outcome can either be \(Y\) or \(\neg Y\). The model estimates outcome likelihood using the logistic function as outlined in equation \eqref{logisticsreg}. Logistic function transforms a real-valued number \(X\) into the range \((0,1)\) which represents the outcome probability \(P(Y)\in(0,1)\).

\begin{equation}
\label{logisticsreg}
P(Y)=\frac{1}{1+e^{-X}}
\end{equation}

\begin{figure}[H]
\centering
<<fig.show='asis', echo=FALSE, eval=TRUE, warning=FALSE, message=FALSE, error=FALSE, fig.width=4, fig.height=2>>=
logit_min <- -5
logit_max <- 5
logistic <- seq(logit_min, logit_max, 0.1)
ggplot(mapping = aes(x=logistic,
                     y=1/(1+exp(-logistic)))) +
  geom_line()+
  labs(x="X", y="Logistic function") +
  scale_colour_brewer(palette="Set2") +
  theme_classic(base_family = "serif")
@
\caption{Graph showing the range of a logistic function}
\end{figure}

In a univariate scenario, the logistic function can be expressed as equation \eqref{logisticprob}, where \(\beta_0\) represents the intercept while \(\beta_1, \beta_2, \beta_3, ..., \beta_M\) refers to the regression coefficient of variables \(x_1, x_2, x_3,...,x_M\). The output \(P(Y)\) indicates the likelihood of true outcome.

\begin{equation}
\label{logisticprob}
P(Y)=\frac{1}{1+e^{-(\beta_0+\beta_1 x_1+\beta_2 x_2+...+\beta_M x_M)}}
\end{equation}

One of the most powerful features of logistic regression is the odds ratio. It quantifies the effects of each independent variable. It is a value indicating the change of likelihood of the event when an independent predictor variable is increased by \(1\) unit. Odds ratio is defined in equation \eqref{oddsratio}.

\begin{equation}
\label{oddsratio}
{OR}(x_1) = \frac{odds(x_1+1)}{odds(x_1)} = \frac{e^{\beta_0+\beta_1 (x_1+1)+\beta_2 x_2+...+\beta_M x_M}}{e^{\beta_0+\beta_1 x_1+\beta_2 x_2+...+\beta_M x_M}} = e^{\beta_1}
\end{equation}

Logistic regression can only handle binary problem. If the dependent variable \(Y\) has more than two outcomes, we can use another algorithm called multinomial logistic regression\footnote{\url{http://www.ats.ucla.edu/stat/r/dae/mlogit.htm}}. Such problem can also be analysed using artificial neural networks in a much more sophisticated way which we will cover in a later section.

\begin{Exercise}[title={Building a Logistic Model}]

In this exercise, we would continue to use the \verb|mtcars| dataset. We will build a logistic regression model to predict whether the vehicle has automatic or manual transmission system (using the \verb|am| variable as dependent variable). 

In the example \ref{ex:logistic}, the model has three independent variables \verb|mpg|, \verb|hp| and \verb|disp|. You may run the example code to build the logistic regression model. You may also calculate the odds ratio and analyse the effects of each variable. The last part of the code is to calculate the model accuracy. 

<<ex:logistic, fig.show='hide', eval=FALSE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize', fig.width=4, fig.height=3, rexample=TRUE>>=
# Build a logistic regression model to predict the dependent variable am
# (1=manual; 0=auto)
myLogisticModel <- glm(am ~ mpg + hp + disp, family="binomial", data=mtcars)
summary(myLogisticModel)
# Calculate the odds-ratios by taking the exponential of the coefficients
# Can you explain the effects of each of these independent variables?
exp(myLogisticModel$coefficients)
# You may also calculate the 95% confidence interval of the odds-ratio
exp(cbind(oddsratio=myLogisticModel$coefficients, confint(myLogisticModel)))
# Returns the modelled probability
myProb <- predict(myLogisticModel, mtcars, type="response")
# Turn the probability into a binary class (i.e. TRUE vs FALSE)
# Probability > 0.5 means the vehicle likely to have manual transmission
myPrediction <- myProb > 0.5
# Construct a contingency table to check correct & incorrect predictions
table(myPrediction, observed=(mtcars$am == 1))
# Calculate model accuracy
# (defined as the percentage of correct prediction)
myAccuracy <- sum(myPrediction==(mtcars$am == 1))/nrow(mtcars)
myAccuracy
@

The logistic model described in example \ref{ex:logistic} can be expressed as equation \eqref{logisticexample}.

\begin{equation}
\label{logisticexample}
\begin{split}
P(manual) & = \frac{1}{1+e^{-(\beta_0 +\beta_1 x_{mpg} +\beta_2 x_{hp} +\beta_3 x_{disp})}} \\
P(automatic) & = P(\neg manual) \\
& = 1 - P(manual)
\end{split}
\end{equation}

\end{Exercise}

\chapter{Tree-based Methods}

Tree-based algorithms belong to the supervised learning discipline. For a given set of labelled objects, decision tree can produce a set of sequential prediction rules based on categorical and numeric predictor variables. It is based on the concept of prediction region (denoted as \(\mathcal{R}_i\)) which refers to a subset of the original object space. Objects situation within the same region share the same prediction.

At the heart of tree-based method is a concept called binary recursive partitioning. It is a top-down process which starts from initial object space. At the first recursion, the algorithm would split the master region \(\mathcal{R}_1\) into two at a cut-off point. This produces two corresponding new regions \(\mathcal{R}_2 \subseteq \mathcal{R}_1\) and \(\mathcal{R}_3 \subseteq \mathcal{R}_1\) with two distinct prediction values. The cutoff point \(s\) determines where to slice the master region. All objects within \(\{X|X_1 < s\}\) belong to \(\mathcal{R}_2\) and those with \(\{X|X_1 \geq s\}\) belong to \(\mathcal{R}_3\). This process runs recursively until it hits the termination criteria.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{img/rpart.PNG}
\caption{Recursive partitioning}
\end{figure}
			
\section{Decision Trees}

Decision tree is the simplest form of any tree-based models. It uses standard recursive partitioning to produce prediction regions. Decision tree is a very generic algorithm which fits both regression and classification problem. This means it can predict both continuous real numbers and discrete categories depending on the type of problem. The region prediction for a classification tree is decided by the majority class, while the prediction for a regression tree is defined as the simple average of all members within the region. The tree-splitting structure is called the topology, which can be interpreted graphically in most cases.

On the downside, recursive partitioning tends to produces very complex trees which may overfit the data. Various control parameters can be used to mitigate the risk of overfitting. For example, recursion can terminate once all regions are small enough to contain less than \(5\) objects.

\begin{Exercise}[title={Growing a Decision Tree}]

In the R language, there are many packages which implement tree-based algorithm. In exercise \ref{ex:decisiontree}, we will use the \verb|rpart| function in the rpart package to build a simple decision trees for a regression problem. The aim of this exercise is to predict car efficiency (\verb|mpg| variable) using the \verb|mtcars| dataset.

<<ex:decisiontree,fig.show='hide', eval=FALSE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize', fig.width=4, fig.height=3, rexample=TRUE>>=
# Load the rpart package for recursive partitioning
# Load the rpart.plot package for tree visualisation
library(rpart)
library(rpart.plot)
# Build a decision tree to predict mpg
myTree <- rpart(formula = mpg ~ wt + hp + factor(carb) + factor(am), 
                data = mtcars,
                control = rpart.control(minsplit=5))
# Read the tree topology
myTree
# Read the detailed summary of the tree
summary(myTree)
# Visualise the decision tree
rpart.plot(myTree)
@

\begin{figure}[H]
\centering
<<fig.show='asis', echo=FALSE, eval=TRUE, warning=FALSE, message=FALSE, error=FALSE, fig.width=8, fig.height=5>>=
library(rpart)
library(rpart.plot)
myTree <- rpart(formula = mpg ~ wt + hp + factor(carb) + factor(am), 
                data = mtcars,
                control = rpart.control(minsplit=5))
rpart.plot(myTree, fallen.leaves = FALSE)
@
\caption{Decision tree for a regression problem}
\end{figure}

\end{Exercise}

\begin{Exercise}[title={Tree Pruning}]

It is a common practice to grow a complex decision tree first, and then decide how to prune it afterwards. Removing weaker branches of the tree usually enhances the model's predictive power which eventually makes it more generalisable.

In exercise \ref{ex:dtreeprune}, the command \verb|printcp(myTree)| returns the relative error of the tree at each and every node. It is defined as \(1-R^2\) and therefore always starts with \(1\) at the top level. As the tree grows, the \(R^2\) value increases and approachs \(1\) therefore the corresponding relative error will diminish towards zero. You can use the function \verb|prune()| to remove weak branches. The complexity parameter \verb|cp| refers to the amount of error reduced when a region is split. We can specify a threshold \verb|cp| value so that branches with weak predictive power can be pruned.

<<ex:dtreeprune, fig.show='hide', eval=FALSE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize', fig.width=4, fig.height=3, rexample=TRUE>>=
# View the cp values
plotcp(myTree)
printcp(myTree)
# Prune the tree at a certain threshold cp value
# You can change the threshold value
?prune
myNewTree <- prune(myTree, cp = 0.03)
rpart.plot(myNewTree, fallen.leaves = FALSE)
@

\end{Exercise}

\section{Random Forest}

Random forest is a collection of many tiny decision trees. All trees in the forest are trained using the same dataset, but with randomly selected predictor variables. In a random forest with \(P\) independent variables, only \(p < P\) variables are randomly selected at each split. Such randomess causes variation among the trees. Some trees would have strong prediction power while some others would be weaker.

Once all the trees are grown, the random forest algorithm combines the output of all trees and uses the simple average as prediction value if it is regression problem. Alternatively if it is a classification problem, the majority label of the region becomes the prediction value.

It is widely recognised that prediction accuracy of random forest is far better than an individual decision tree. However as a trade-off, random forest is often harder to interpret manually as the decision rule becomes more complicated.

\begin{Exercise}[title={Planting a Random Forest}]

Example \ref{ex:randomforest} demonstrates how to train a random forest model. Each tree in the forest would randomly select a few variables for assessment. You can use the \verb|randomForest| package to build random forest rapidly.

The importance of each predictor variable is indicated by the decrease of node impurity. A powerful predictor would substantially decrease node impurity. For regression, it is measured by residual sum of squares. For classification, the node impurity is measured by the Gini index. You can use the function \verb|importance(myForest)| or \verb|varImpPlot(myForest)| to calculate the importance measurement.

<<ex:randomforest, fig.show='hide', eval=FALSE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize', fig.width=4, fig.height=3, rexample=TRUE>>=
library(randomForest)
library(dplyr)
# Build a random forest with 1000 trees
# Each tree has 2 randomly selected variables
# You can change the parameters
myForest <- randomForest(mpg ~ wt + hp + carb + am, 
                         ntree = 1000,
                         mtry = 2,
                         data = mtcars %>% mutate(carb = factor(carb), 
                                                  am = factor(am)))
# Plot the error as the forest expands
plot(myForest)
# Plot the distribution of tree size
treesize(myForest) %>% hist()
# Model summary
myForest
# Relative importance of each independent variable
importance(myForest)
varImpPlot(myForest)
@
\end{Exercise}


\chapter{Neural Networks}

Artificial neural networks (ANN) are mathematical algorithms inspired by the structure of the biological brains. It process incoming information through non-linear mechanism in a neuron and pass on the output to another neuron. When this process repeats many times via multiple layers of neurons, it becomes an artificial neural network. Neural networks having complex structure are usually trained iteratively using backpropagation techniques over long period of time with massive computational power. Nowadays, many modern applications are based on state-of-the-art neural networks, such as video analysis, speech recognition, chatbots and machine translation.

In an ANN, each hidden neuron carries a non-linear activation function \(f(x)\). Sigmoid function is a traditional choice of activation function for ANNs\eqref{sigmoid}. It takes the weighted sum of input plus the bias unit and squashes everything into the range \((0,1)\) with a characteristic sigmoidal 'S' shape. As the sigmoid function is differentiable and easy to compute, it soon becomes a popular choice for ANN activation function. However, it suffers from weak gradient when the input is far away from zero (i.e. the neuron saturates), which makes the ANN learn very slow. 

To address the problem of weak gradient, alternative activation functions have been proposed. For instance, the hyperbolic tangent function can be used\eqref{tanh}. It shares the same sigmoidal shape but further stretches the output to the range \((-1,1)\) therefore provides stronger gradient. Yet, the gradient still suffers from saturation when the input is too small or too large.

Different activation functions can provide stronger gradients while maintaining non-linearity. For instance, the softplus function has strong gradient (i.e. unsaturable) for any positive input \eqref{softplus}. However, it has been considered computationally costly as it contains logarithm and exponential terms. In light of this, a simplified version call rectified linear unit (ReLU) is usually used instead \eqref{relu}. The shape of ReLU is very similar to softplus with the exception that it has a threshold at zero. This means only positive input can lead to activation. However, the weighted sum input can change to negative value during training therefore causing the ReLU neuron to cease training. This is called the dying ReLU problem. To avoid this, more advanced activation functions incorporate a very small gradient in the negative range to allow the neuron to recover. The output of common activation functions are visualised in figure \ref{fig:activation}.
	
\begin{subequations}
	Sigmoid activation
	\begin{equation}
	\label{sigmoid}
	f(x)= \frac{1}{1+e^{-x}}
	\end{equation}
	
	Hyperbolic tangent activation
	\begin{equation}
	\label{tanh}
	f(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}
	\end{equation}
	
	Softplus activation
	\begin{equation}
	\label{softplus}
	f(x)= \ln(1+e^x)
	\end{equation}
	
	Rectified linear unit (ReLU)
	\begin{equation}
	\label{relu}
	f(x)= \max(0,x)
	\end{equation}
	
\end{subequations}


\begin{figure}[H]
\centering
<<fig.show='asis', echo=FALSE, eval=TRUE, warning=FALSE, message=FALSE, error=FALSE, fig.width=7, fig.height=4>>=
range <- seq(-3,3,0.05)

func_sigmoid <- 1/(1+exp(-range))
func_tanh <-tanh(range)
func_softplus <- log(1+exp(range))
func_relu <- sapply(range, max, 0)

tibble(func="Sigmoid", value= func_sigmoid, range = range) %>%
  union_all(tibble(func="Hyperbolic tangent", value= func_tanh, range = range)) %>%
  union_all(tibble(func="Softplus", value= func_softplus, range = range)) %>%
  union_all(tibble(func="ReLU", value= func_relu, range = range)) %>%
  union_all(tibble(func="Linear", value= range, range = range)) %>%
  ggplot(aes(x=range, y=value, colour=func)) +
  geom_line() +
  scale_colour_brewer(palette="Set2") +
  theme_classic(base_family = "serif") + 
  theme(legend.position = "right") +
  labs(x="x", y="f(x)", colour="Activation Function")
@
\caption{Common neural activation functions}
\label{fig:activation}
\end{figure}

The output prediction is made at the network's final layer. Each neuron at this layer combines the hidden neuron's activation though weighted sum and a bias adjustment unit \eqref{weightedhidden}. For regression problems, a linear activation layer is usually used to map the output vector back into unbounded real value range \eqref{linear}. For classification problem, softmax function is used as the final output layer. It maps the hidden layer's activations into the range \((0,1)\) where the sum of the entire output vector is restricted to\(1\) in order to represent class probabilities \eqref{softmax}.

\begin{subequations}
	Weighted sum of hidden vector with bias adjustment
	\begin{equation}
	\label{weightedhidden}
	y_k=\beta_k+\sum_{h=1}^{H} w_{h,k}h_h , k=1,...,K
	\end{equation}
	
	Linear output
	\begin{equation}
	\label{linear}
	\hat{Y_k} = y_k
	\end{equation}
			
	Softmax output
	\begin{equation}
	\label{softmax}
	\hat{Y_k} = \frac{e^{y_k}}{ \sum_{k^\prime=1}^{K} e^{y_{k^\prime}} }
	\end{equation}
\end{subequations}

\section{Multilayer Perceptron}

A multilayer perceptron (MLP) is the simplest form of all neural network. It consists of several stacked hidden layers, where every neurons in the hidden layers are fully interconnected.

It is common practice to use zero-centred values for neural network training\footnote{Zero-centred values have stronger gradient, thus speed up optimisation through gradient descent.}. To achieve this, you can normalise variables into \(z\)-score \eqref{zscore} so that they have similar range. For any individual value \(x_i\), the \(z\)-score can be calculated as the distance from the arithmetic mean \(\bar{x}\) divided by standard deviation \(\sigma\) of the variable.

\begin{subequations}
	\begin{equation}
	\label{zscore}
	z_i = \frac{x_i - \bar{x}}{\sigma}
	\end{equation}
\end{subequations}

At the training phase, network weights are usually initialised randomly. They are then optimised through backpropagation to achieve gradent descent. The weights improves gradually according to a predefined learning rate until they ultimately converge at the minimum value. One of the drawbacks is that backpropagation does not guarantee reaching global minimum if there are multiple minima across the parameter space. Such problem is usually mitigated by using advanced optimisers with adaptive learning rate.

\begin{Exercise}[title={Training MLP for Regression Problem}]

There are many packages which implements neural network in the R language. Traditional packages include \verb|neuralnets|, \verb|nnet|, \verb|RSNNS|, \verb|caret|... etc. Modern deep learning frameworks such as \verb|h2o|, \verb|MXNet| and \verb|kerasR| are also available in R, but they usually require premium hardware set-up. In general, all neural network packages implement the same underlying algorithm and the difference usually lies in syntax, execution speed and hardware compatibility.

We will continue to use the \verb|mtcars| dataset in this exercise. The objective of this exercise is to predict the \verb|mpg| value of each car given all other known attributes of it. We would use the \verb|neuralnet| package to create a simple multilayer perceptron (MLP) model. Code in example \ref{ex:neuralnet} trains a fully-connected multilayer perceptron with two hidden layers.

<<ex:neuralnet, fig.show='hide', eval=TRUE, results='hide',warning=FALSE, message=FALSE, error=FALSE, size='scriptsize', fig.width=7, fig.height=5, rexample=TRUE>>=
library(dplyr)
library(neuralnet)
# The mtcars dataset has a mixture of numeric and categorical variables
# For numeric variables we need to normalise them
mtcars_numeric <- mtcars %>% select(mpg, disp, hp, drat, wt, qsec)
# foreach numeric variable, we calculate the mean and standard deviation
mtcars_mean <- mtcars_numeric %>% lapply(mean)
mtcars_sd <- mtcars_numeric %>% lapply(sd)
# Convert the numeric variables into z-scores using the mean and sd
mtcars_numeric_normalised <- (mtcars_numeric - mtcars_mean) / mtcars_sd
# Construct a two layers MLP using all numeric variables.
# By default it uses sigmoid active function
myNN1 <- neuralnet(formula = mpg ~ disp + hp + drat + wt + qsec,
                   data = mtcars_numeric_normalised,
                   hidden = c(4,3),
                   linear.output = TRUE,
                   lifesign = "full")
# Visualise the network topology
plot(myNN1)
@

Example \ref{ex:neuralnet2} shows that you can load the package \verb|NeuralNetTools| to create better topology plot. In addition, you can plot the observed data against network predictions to visualise the error. For a regression problem, mean squared error (MSE) defined as \( \frac{1}{N} \sum_{n=1}^{N} (\hat{y}_n-y_n)^2 \) is usually used as the error measurement.

<<ex:neuralnet2, fig.show='hide', eval=TRUE, results='hide',warning=FALSE, message=FALSE, error=FALSE, size='scriptsize', fig.width=7, fig.height=5, rexample=TRUE>>=
# Use a helper package for prettier graphics (optional)
library(NeuralNetTools)
plotnet(myNN1)
# Calculate the network prediction
myNNResult1 <- compute(myNN1, mtcars_numeric_normalised %>% select(-mpg))
# The predicted values are in scaled format (z-score)
# Need to convert it back to original scale for comparison
myNNPred1 <- myNNResult1$net.result[,1] *
                mtcars_sd[["mpg"]] + 
                mtcars_mean[["mpg"]]
# Visualise the results on a scatterplot
qplot(mtcars$mpg, myNNPred1) +
  labs(x="Observed MPG",
       y="Predicted MPG")
# Calculate model error using mean squared error (MSE)
myNNError1 <- mean((myNNPred1 - mtcars$mpg)^2)
@

\begin{figure}[H]
\centering
<<fig.show='asis', echo=FALSE, eval=TRUE, warning=FALSE, message=FALSE, error=FALSE, fig.width=7, fig.height=4>>=
plot(myNN1, rep="best")
@
\caption{MLP model with two hidden layers}
\end{figure}

Sigmoid function is traditionally used in shallow networks. It can suffer from weak gradient when stacked in deep networks. You can use the code in example \ref{ex:neuralnetactivation} to customise the activation function.

<<ex:neuralnetactivation, fig.show='hide', eval=TRUE, results='hide',warning=FALSE, message=FALSE, error=FALSE, size='scriptsize', fig.width=7, fig.height=5, rexample=TRUE>>=
# Build the second model
# Keep everything the same but change to softplus activation
myNN2 <- neuralnet(formula = mpg ~ disp + hp + drat + wt + qsec,
          data = mtcars_numeric_normalised,
          hidden = c(4,3),
          linear.output = TRUE,
          act.fct = function(x) { log(1+exp(x)) },
          lifesign = "full")
# Calculate the network prediction
myNNResult2 <- compute(myNN2, mtcars_numeric_normalised %>% select(-mpg))
# Convert the predicted values back to original scale
myNNPred2 <- myNNResult2$net.result[,1] *
                mtcars_sd[["mpg"]] + 
                mtcars_mean[["mpg"]]
# Calculate model error (MSE)
myNNError2 <- mean((myNNPred2 - mtcars$mpg)^2)
@

Neural networks can also deal with categorical inputs. They are usually converted into one-hot encoding to feed into the model\footnote{For a categorical variable with \(K\) unique values, one-hot encoding would produce \(K\) new variables. Each new variables would have value \(\{1,0\}\). Please note that this is different from dummy encoding in statistical modelling.}. Code in example \ref{ex:neuralnetencode} converts all categorical variables in the \verb|mtcars| dataset into one-hot encoding. The encoded values afre then binded to the numeric values and jointly used for training.

<<ex:neuralnetencode, fig.show='hide', eval=TRUE, results='hide',warning=FALSE, message=FALSE, error=FALSE, size='scriptsize', fig.width=7, fig.height=5, rexample=TRUE>>=
# Loads the purrr package to access the imap function
library(purrr)
# Selecting all the categorical variables in the dataset
# Use the imap function to iterate through all columns
# Convert all into one-hot encoding and binds back into a tibble
mtcars_encoded <- mtcars %>% select(cyl,vs,am,gear,carb) %>% 
  imap(function(myCol, myName) {
    myUniqueValues <- unique(myCol)
    myTib <- sapply(myUniqueValues, 
           function(myValue){ (myValue == myCol) * 1 }) %>% as_tibble
    colnames(myTib) <- paste0(myName, "_", myUniqueValues)
    return(myTib)
  }) %>% bind_cols()
# Combines all numeric and categorical variables
mtcars_all <- bind_cols(mtcars_numeric_normalised, mtcars_encoded)
# View the dataset
View(mtcars_all)
# Train the third model by including encoded categorical variables
myNN3 <- neuralnet(formula = mpg ~ 
                     disp + hp + drat + wt + qsec +
                     cyl_6 + cyl_4 + cyl_8 + 
                     vs_0 + vs_1 + 
                     am_1 + am_0 + 
                     gear_4 + gear_3 + gear_5 + 
                     carb_4 + carb_1 + carb_2 + carb_3 + carb_6 + carb_8,
          data = mtcars_all,
          hidden = c(4,3),
          linear.output = TRUE,
          act.fct = function(x) { log(1+exp(x)) },
          lifesign = "full")
# Visualise the network topology
plot(myNN3)
# Calculate the network prediction
myNNResult3 <- compute(myNN3, mtcars_all %>% select(-mpg))
# Convert the predicted values back to original scale
myNNPred3 <- myNNResult3$net.result[,1] *
                mtcars_sd[["mpg"]] + 
                mtcars_mean[["mpg"]]
# Calculate model error (MSE)
myNNError3 <- mean((myNNPred3 - mtcars$mpg)^2)
# Compare the error of the three models
myNNError1
myNNError2
myNNError3
@

\begin{figure}[H]
\centering
<<fig.show='asis', echo=FALSE, eval=TRUE, warning=FALSE, message=FALSE, error=FALSE, fig.width=7, fig.height=4>>=
plot(myNN3, rep="best")
@
\caption{MLP model with one-hot encoded categorical variables}
\end{figure}

\end{Exercise}


\chapter{Time Series Analysis}

Many datasets have temporal dimension. Time series refers to a chronologically-ordered sequence of observations. There are two main types of time series data: 
\begin{inparaenum}[1)]
\item regularly-sampled\footnote{Regularly sampled time series has observations taken at fixed interval. This includes examples like heart rate, network traffic, daily weather... etc.}, and
\item irregularly-sampled\footnote{Refers to when observations are not recorded at fixed interval, such as incidents of earthquakes.}
\end{inparaenum}
In this chapter, we would focus on time series data sampled at regularly-spaced intervals.

Temporal properties are often the point-of-interest in time series analysis. This include trend, seasonality, or temporal dependency between different variables. Studying these properties can gain useful insights. For example, extrapolating trend can create forecast for the future. Alternatively, analysing seasonality can help users understand the nature of recurring patterns.

\section{Auto-Correlation Function}

Auto-correlation function (ACF) measures the correlation of a single variable along the temporal dimension between \(x_t\) and \(x_{t+h}\). In other words, it shows the correlation of the variable over different lag periods. 

In the R language, you may use the \verb|Acf(x)| function within the package \verb|forecast| to plot the ACF correlogram. For most time series variables, correlation is usually strong at lag \(h=1\) and it gradually diminishes as lag period increases. Cyclic pattern in the correlogram suggests possible seasonality which you can analyse further.

On the other hand, the partial auto-correlation function (PACF) is similar to the ACF in the sense that it also measures the correlation between different lag periods. The difference is that it controls the correlation across the temporal dimsnion so that only the contribution of an individual lag is reflected.

\begin{Exercise}[title={Loading Datasets}]

In this exercise, we would use an external dataset. This dataset contains daily electricity generation and demand data published by a German transmission network called Amprion\footnote{Amprion - Demand in Conrrol Area \url{https://www.amprion.net/Grid-Data/Demand-in-Control-Area/}}. The first column is a date time variable and the rest are demand and generation data, each sampled at \(15\) minutes granularity. Code in example \ref{ex:loadamprion} show how to load the dataset from CSV file.

\begin {table}[H]
\caption {Description of the Amprion dataset}
\begin{center}
\begin{tabular}{ | l | l | l |}
\toprule[1.5pt]
{\bf Variable}            & \bf{Measurement Unit}           & {\bf Description}\\
\midrule
\verb|datetime|           & \verb|%Y-%m-%d %H:%M:%S|        & Date and time\\
\verb|demand|             & Megawatt                        & Demand in control area\\
\verb|pv|                 & Megawatt                        & Photovoltaic feed-in\\
\verb|wp|                 & Megawatt                        & Wind power feed-in\\
\bottomrule[1.5pt]
\end{tabular}
\end{center}
\end{table}

<<ex:loadamprion, fig.show='asis', eval=TRUE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize', fig.width=4, fig.height=3, rexample=TRUE>>=
# Read the Amprion dataset from CSV file
# You might have to modify the path to point to your file location
amprion <- read.csv("amprion.csv",
                    colClasses = c("character",
                                   "numeric",
                                   "numeric",
                                   "numeric")) %>% as_tibble()
# View the dataset
amprion
@

One of the main drivers of power demand and generation is weather. The weather dataset is published by the Deutscher Wetterdienst\footnote{DWD Climate Data Center \url{https://www.dwd.de/EN/climate_environment/cdc/cdc_node.html}}. Weather observations are recorded every hour at the Bremen weather station. You can follow the code in example \ref{ex:loadbremen} to load the dataset.

\begin {table}[H]
\caption {Description of the Bremen weather dataset}
\begin{center}
\begin{tabular}{ | l | l | l |}
\toprule[1.5pt]
{\bf Variable}            & \bf{Measurement Unit}     & {\bf Description}\\
\midrule
\verb|datetime|           & \verb|%Y-%m-%d %H:%M:%S|  & Date and time\\
\verb|airtemp|            & Degree Celsius            & Air temperature\\
\verb|sun|                & \(J {cm}^-1\)             & Short-wave global radiation\\
\verb|windspd|            & \(m {sec}^-1\)            & Wind speed\\
\verb|winddir|            & Bearing                   & Wind direction\\
\verb|soil10|             & Degree Celsius            & Soil temperature at \(10\)cm depth\\
\verb|soil20|             & Degree Celsius            & Soil temperature at \(20\)cm depth\\
\verb|soil50|             & Degree Celsius            & Soil temperature at \(50\)cm depth\\
\verb|soil100|            & Degree Celsius            & Soil temperature at \(100\)cm depth\\
\bottomrule[1.5pt]
\end{tabular}
\end{center}
\end{table}

<<ex:loadbremen, fig.show='asis', eval=TRUE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize', fig.width=4, fig.height=3, rexample=TRUE>>=
# Load the Bremen weather dataset
bremen <- read.csv("bremen.csv",
                   colClasses = c("character",
                                  "numeric",
                                  "numeric",
                                  "numeric",
                                  "factor",
                                  "numeric",
                                  "numeric",
                                  "numeric",
                                  "numeric")) %>% as_tibble()
# View the dataset
bremen
@

Before moving on to further analysis, we need to aggregate the two datasets into the same granularity first. Once this is done, we can then join the two datasets together to form one table containing all variables. Example \ref{ex:joinamprionbremen} shows how to use SQL-like pipeline in \verb|dplyr| to aggregate and join the two datasets.

<<ex:joinamprionbremen, fig.show='hide', eval=TRUE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize', fig.width=4, fig.height=3, rexample=TRUE>>=
# Load the lubridate package to access more datetime functions
library(lubridate)
# Load the dplyr package for data wrangling
library(dplyr)
# Aggregate the amprion dataset from 15 minutes to daily level.
amprion_daily <- amprion %>%
                  mutate(date = datetime %>% 
                           ymd_hms() %>% 
                           floor_date("day") %>%
                           as.Date()) %>%
                  group_by(date) %>%
                  summarise(total_demand = sum(demand),
                            total_pv = sum(pv),
                            total_wp = sum(wp))
# Aggregate the bremen dataset from hourly to daily.
bremen_daily <- bremen %>%
                  mutate(date = datetime %>% 
                           ymd_hms() %>% 
                           floor_date("day") %>%
                           as.Date()) %>%
                  group_by(date) %>% 
                  summarise(mean_airtemp = airtemp %>% mean(),
                            max_sun = sun %>% max(),
                            mean_windspd = windspd %>% mean(),
                            mean_soil10 = soil10 %>% mean(),
                            mean_soil20 = soil20 %>% mean(),
                            mean_soil50 = soil50 %>% mean(),
                            mean_soil100 = soil100 %>% mean())
# Join the two daily datasets together into a common table
myTable <- amprion_daily %>%
  left_join(bremen_daily, by = "date")
# View the aggregated datasets
View(myTable)
# Plots the daily total demand
myTable %>%
  ggplot(aes(x=date, y=total_demand)) + 
  geom_line() +
  labs(x = "Date",
       y = "Power Demand (MW)")
@

\end{Exercise}

\begin{Exercise}[title={Analysing Temporal Correlation}]

You can use the code in example \ref{ex:acf} to create the ACF and PACF correlograms. In addition, you can use the \verb|Ccf()| or \verb|ggCcf()| function to create a cross correlation function (CCF) correlogram. It analyses the temporal correlation between two variables.

<<ex:acf, fig.show='hide', eval=FALSE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize', fig.width=4, fig.height=3, rexample=TRUE>>=
# Load the forecast package
library(forecast)
# Plots the ACF correlogram only
# There are several ways to create plots.
ggAcf(myTable$total_demand) # More pretty
Acf(myTable$total_demand)   # Standard base R plot
# Plots the PACF correlogram only.
ggPacf(myTable$total_demand)
Pacf(myTable$total_demand)
# Draw a CCF correlogram which find the correlation between two variables.
# You can try swapping variables here.
ggCcf(x = myTable$mean_airtemp, 
      y = myTable$total_demand)
Ccf(x = myTable$mean_airtemp, 
    y = myTable$total_demand)
# Constructs the several key plots in one go.
ggtsdisplay(myTable$total_demand)
tsdisplay(myTable$total_demand)
# Create a lag plot 
gglagplot(myTable$total_demand, lags = 28)
lag.plot(myTable$total_demand, lags = 28)
@

\begin{figure}[H]
\centering
<<fig.show='asis', echo=FALSE, eval=TRUE, warning=FALSE, message=FALSE, error=FALSE, fig.width=8, fig.height=6>>=
library(forecast)
tsdisplay(myTable$total_demand, 
          main = "Total daily power demand (MW)",
          points = FALSE,
          lag.max = 120)
@
\caption{ACF and PACF correlograms}
\end{figure}


\begin{figure}[H]
\centering
<<fig.show='asis', echo=FALSE, eval=TRUE, warning=FALSE, message=FALSE, error=FALSE, fig.width=7, fig.height=5>>=
library(forecast)
lag.plot(myTable$total_demand,lags = 28, layout = c(4,7))
@
\caption{Lag plot showing the correlation of various lag periods}
\end{figure}

\end{Exercise}

\section{Decomposition}
Time series can be either additive or multiplicative. Additive time series can be generally described as \(X_t=S_t+T_t+\epsilon_t\) where \(S_t\) refers to the seasonality at time \(t\) while \(T_t\) refers to the trend component. The observed data \(X_t\) is simply the sum total of trend, seasonal and error components. Alternative, a multiplicative time series is defined as \(X_t = S_t \times T_t \times \epsilon_t\). These components can be easily decomposed from the observed values.

\begin{Exercise}[title={Identifying Trend and Seasonal Components}]

To analyse the seasonality of a time series, you need to find out the ideal frequency of the seasonal component. Example \ref{ex:frequency} uses the function \verb|findfrequency()| to identify the frequency for a given time series. It uses spectal analysis to identify the frequency with strongest spectral density. Once the frequency is calculated, we can build a \verb|ts| object using the calculated frequency value. The function \verb|decompose()| converts the observed time series into trend component \(T_t \in [1,T]\), seasonal component \(S_t \in [1,T]\) and random residuals \(\epsilon_t \in [1,T]\). 

The code also divides the dataset into training and testing set. The training set is used to run analysis and train models. Once models are trained, they are applied to the testing set to assess model performance.

<<ex:frequency, fig.show='hide', eval=TRUE, results='hide', warning=FALSE, message=FALSE, error=FALSE, size='scriptsize', fig.width=7, fig.height=5, rexample=TRUE>>=
# Divide the dataset into training and testing set
TEST_SET_BEGIN <- "2017-01-01"
myTrainingSet <- myTable %>% filter(date < TEST_SET_BEGIN)
myTestingSet <- myTable %>% filter(date >= TEST_SET_BEGIN)
# Automatically search for ideal frequency using training data
# We would expect the frequency to be 7 (weekly pattern)
myFreq <- findfrequency(myTrainingSet$total_demand)
# Check the calculated frequency
myFreq
# Define a seasonal time series object using the frequency value
myTs <- ts(data = myTrainingSet$total_demand,
           frequency = myFreq)
# Decompose the time series into its constituent components
myDecomp <- decompose(myTs,
                      type = "additive")
# View the decomposed components
autoplot(myDecomp)
plot(myDecomp)
@

\begin{figure}[H]
\centering
<<fig.show='asis', echo=FALSE, eval=TRUE, warning=FALSE, message=FALSE, error=FALSE, fig.width=7, fig.height=5>>=
plot(myDecomp)
@
\caption{Decomposing an additive time series}
\end{figure}

\end{Exercise}

\begin{Exercise}[title={Linear Time Series Forecast}]

In this exercise, we will build a simple forecasting model using the trend and seasonal components as independent variables. The mathematical formula of a linear model with \(M\) predictor variables can be expressed as \eqref{tslm}. Code in example \ref{ex:tslm} shows how to build a time series linear regression model with several covariate variables as predictors. It also visualises the forecast output as a chart. The coloured area surrounding the line represents the confidence interval of your prediction.

\begin{equation}
\label{tslm}
X_t = \beta_0 + \beta_{trend} T_t + \beta_{seasonal}S_t + \sum_{m=1}^{M}(\beta_m {x_m}_t) + \epsilon_t
\end{equation}

<<ex:tslm, fig.show='hide', eval=TRUE, results='hide',warning=FALSE, message=FALSE, error=FALSE, size='scriptsize', fig.width=7, fig.height=5, rexample=TRUE>>=
library(forecast)
# Perform linear regression model with decomposed time series components
# You can also add interaction and polynomial terms here
myTsModel1 <- tslm(myTs ~ trend + season + 
                     mean_airtemp * mean_windspd + 
                     poly(max_sun,degree = 2) + 
                     mean_soil10 + mean_soil20,
                   data = myTrainingSet)
# View model summary
summary(myTsModel1)
# Produce forecast using the testing set
myTsForecast1 <- forecast(object = myTsModel1, 
                          newdata = myTestingSet)
# Visualise the forecast
autoplot(myTsForecast1)
plot(myTsForecast1)
# Calculate the model performance by comparing with the testing set
# Using mean squared error (MSE) here.
myTestError1 <- mean((myTsForecast1$mean - myTestingSet$total_demand)^2)
@

\begin{figure}[H]
\centering
<<fig.show='asis', echo=FALSE, eval=TRUE, warning=FALSE, message=FALSE, error=FALSE, fig.width=7, fig.height=3>>=
plot(myTsForecast1)
@
\caption{Linear time series forecasting with trend and seasonal components}
\end{figure}

To compare the time series regression model output with standard regression, you can run a simple linear regresion model using the same set of predictor variables with the \verb|lm()| function. This is shown in example \ref{ex:tslmvslm}.

<<ex:tslmvslm, fig.show='hide', eval=TRUE, results='hide', warning=FALSE, message=FALSE, error=FALSE, size='scriptsize', fig.width=7, fig.height=5, rexample=TRUE>>=
myTsModel2 <- lm(myTs ~ mean_airtemp * mean_windspd + 
                 poly(max_sun,degree = 2) + 
                 mean_soil10 + mean_soil20,
                 data = myTrainingSet)
# View model summary
summary(myTsModel2)
# Calculate model prediction using testing set
myTsForecast2 <- predict(object = myTsModel2, 
                         newdata = myTestingSet)
# Calculate testing MSE
myTestError2 <- mean((myTsForecast2 - myTestingSet$total_demand)^2)
# Compare the testing error (MSE) of these two models
# Which one is a better model?
myTestError1
myTestError2
# Visualise the predictions of the two models
ggplot() +
  geom_line(aes(x=date, y=total_demand), myTrainingSet) + 
  geom_line(aes(x=date, y=myTsForecast1$mean, colour="blue"),
            myTestingSet) +
  geom_line(aes(x=date, y=myTsForecast2, colour="green"),
            myTestingSet) + 
  scale_colour_manual(guide = "legend", name = "Model", 
         values =c("blue"="blue",
                   "green"="green"), 
         labels = c("Time Series Regression Model",
                    "Simple Linear Regression")) + 
  labs(x="Date",
       y="Total Demand (MW)") +
  theme(legend.position = "bottom")
@

\end{Exercise}

\section{ARIMA Model}

ARIMA is the acronym for Auto-Regressive Integrative Moving Average model. It is a statistical technique which incorporates lag within the model. It can be described as the combination of three separate parts: autoregression, integration and moving average. ARIMA has three corresponding parameters \(p\),\(d\),and \(q\) which is normally expressed as \(ARIMA(p,d,q)\) or as separate terms \(AR(p)\), \(I(d)\) and \(MA(q)\).

The \(AR(p)\) part suggests that observation \(X_t\) is dependent on the linear combination of lagged terms up to \(p\) lag periods. A pure \(AR(p)\) model is expressed as \(X_t=\sum_{i=i}^{p}(\phi_i X_{t-i})\). The moving average part \(MA(q)\) indicates the residual is inherited from up to \(q\) lag periods. A pure \(MA(q)\) model can be expressed as as \(X_t=\sum_{i=1}^{q}(\theta_i \epsilon_{t-i}) + \epsilon_t \). As a result, a simple \(ARMA(p,q)\) model can be expressed as the following where \(\phi_i\) and \(\theta_i\) are model coefficients, while \(X_{t-i}\) represent observed data at \(i^{th}\) lag step and \(\epsilon_{t-i}\) refers to the random error at the \(i^{th}\) lag step \eqref{arma}.

\begin{equation}
\label{arma}
\underbrace{X_t}_\text{Observation} = 
\underbrace{ \beta_0 }_\text{intercept}+ 
\underbrace{ \sum_{i=i}^{p}(\phi_i X_{t-1}) }_\text{AR(p)} + 
\underbrace{ \sum_{i=1}^{q}(\theta_i \epsilon_{t-i}) }_\text{MA(q)} + 
\underbrace{\epsilon_t}_\text{residual}
\end{equation}

ARIMA model assumes the time series conforms stationarity\footnote{A stationary time series has consistent statistical properties across all time, such as equal mean and variance.}. The integrative component \(I(d)\) ensures stationarity by taking \(d\) number of integrative steps over time. A first order integrative model \(I(1)\) is simply the difference between current step and the immediate previous lag step. It is expressed as \( X_t^{'}=X_t - X_{t-1} \). Similarly, a second order integrative model \(I(2)\) is expressed as \( X_t^{''} = X_t^{'} - X_{t-1}^{'} = X_t - 2X_{t-1}+X_{t-2} \).

Time series data with seasonality can be expressed as \(ARIMA(p,d,q)(P,D,Q)_m\) where the uppercase parameters represent the seasonal component of the model. The \(m\) value is a positive non-zero integer indicating the frequency of the seasonality. The estimates \(AR(P)\), \(I(D)\) and \(MA(Q)\) are linearly combined together with the non-seasonal part to create the seasonal ARIMA (SARIMA) model.

\begin{Exercise}[title={Automated ARIMA}]

The standard ARIMA implementation accepts six parameters \(p\), \(d\), \(q\), \(P\), \(D\) and \(Q\) which produces a seasonal time series model. In many cases, these values are usually not known to the user and all possible values are examined case-by-case to get the best fit.

In the \verb|forecast| package\footnote{The package author has published a detailed book: \url{https://www.otexts.org/fpp/}}, you may use the function \verb|Arima()| to experiment parameters manually. Alternatively, it is quite common to use automated method to search for good parameters. The method \verb|auto.arima()| tries all parameter values within the given constraints. It can also fit linear regression using predictor variables if the \verb|xreg| attribute is supplied to the function. This is considerably slower than the \verb|Arima()| function due to overhead for parameter search. Example \ref{ex:autoarima} demonstrates parameter searching using automated ARIMA.

<<ex:autoarima, fig.show='hide', eval=FALSE, results='hide',warning=FALSE, message=FALSE, error=FALSE, size='scriptsize', fig.width=7, fig.height=5, rexample=TRUE>>=
library(forecast)
library(dplyr)
# Build an ARIMA model automatically
# Keeping the maximum order (p+d+P+D) small
# Search for seasonal model only
myTsModel3 <- auto.arima(y = myTs,
                         max.order = 5,
                         seasonal = TRUE,
                         xreg = myTrainingSet %>% 
                                 select(mean_airtemp, 
                                        mean_windspd, 
                                        max_sun,
                                        mean_soil10,
                                        mean_soil20,
                                        mean_soil50,
                                        mean_soil100),
                         trace = TRUE)
# View the ARIMA(p,d,q)(P,D,Q) estimates and their coefficients
summary(myTsModel3)
# Run the forecast
# Apply the ARIMA model to testing set
myTsForecast3 <- forecast(myTsModel3,
                          xreg = myTestingSet %>%
                                   select(mean_airtemp, 
                                          mean_windspd, 
                                          max_sun,
                                          mean_soil10,
                                          mean_soil20,
                                          mean_soil50,
                                          mean_soil100))
# Visualise the forecast
autoplot(myTsForecast3)
plot(myTsForecast3)
# Calculate the MSE error using the testing set
myTestError3 <- mean((myTsForecast3$mean - myTestingSet$total_demand)^2)
@
\end{Exercise}

\begin{Exercise}[title={Custom ARIMA}]

After running the automated ARIMA forecast, you might realise the forecast tends to flat out when forecast horizon increases. This is because the \verb|auto.arima()| function selects the best parameters based on an indicated called AIC. It maximises the log-likelihood of the training data and gives preference to simpler models. We can manually tweak the ARIMA model with custom parameters using the \verb|Arima()| function instead. This is demonstrated in example \ref{ex:customarima}.

<<ex:customarima, fig.show='hide', eval=FALSE, results='hide',warning=FALSE, message=FALSE, error=FALSE, size='scriptsize', fig.width=7, fig.height=5, cache=TRUE, rexample=TRUE>>=
# Use custom parameters for the ARIMA model
# In this case we can try ARIMA(2,0,0)(1,1,1)
# You can change the parameters here
myTsModel4 <- Arima(y = myTs, 
                    xreg = myTrainingSet %>% 
                            select(mean_airtemp,
                                   mean_windspd,
                                   max_sun,
                                   mean_soil10,
                                   mean_soil20,
                                   mean_soil50,
                                   mean_soil100),
                    order = c(2,0,0), 
                    seasonal = c(1,1,1)) 
# View the model summary
summary(myTsModel4)
# Apply the ARIMA model to test set
myTsForecast4 <- forecast(myTsModel4, 
                          xreg = myTestingSet %>% 
                            select(mean_airtemp,
                                   mean_windspd,
                                   max_sun,
                                   mean_soil10,
                                   mean_soil20,
                                   mean_soil50,
                                   mean_soil100))
# Visualise the forecast
autoplot(myTsForecast4)
plot(myTsForecast4)
# Calculate MSE error using the testing set
myTestError4 <- mean((myTsForecast4$mean - myTestingSet$total_demand)^2)
@

\begin{figure}[H]
\centering
<<fig.show='asis', echo=FALSE, eval=TRUE, warning=FALSE, message=FALSE, error=FALSE, fig.width=7, fig.height=3>>=
myTsModel4 <- Arima(y = myTs, 
                    xreg = myTrainingSet %>% 
                            dplyr::select(mean_airtemp,
                                          mean_windspd,
                                          max_sun,
                                          mean_soil10,
                                          mean_soil20,
                                          mean_soil50,
                                          mean_soil100),
                    order = c(2,0,0), 
                    seasonal = c(1,1,1)) 
myTsForecast4 <- forecast(myTsModel4, 
                          xreg = myTestingSet %>% 
                            dplyr::select(mean_airtemp,
                                         mean_windspd,
                                         max_sun,
                                         mean_soil10,
                                         mean_soil20,
                                         mean_soil50,
                                         mean_soil100))
plot(myTsForecast4)
@
\caption{Forecast generated from a seasonal ARIMA model.}
\end{figure}

\end{Exercise}


\begin{Exercise}[title={Model-based Simulation}]

With a full \(ARIMA(p,d,q)(P,D,Q)_m\) model, model-based simulation can be created very easily. The code in example \ref{ex:arimasim} will generate \(100\) simulated runs and plot the average of all runs as point forecast on a chart.

<<ex:arimasim, fig.show='hide', eval=FALSE, results='hide',warning=FALSE, message=FALSE, error=FALSE, size='scriptsize', fig.width=7, fig.height=5, rexample=TRUE>>=
# Use one of the trained ARIMA model for simulation
# Wrapping the simulation in a lapply loop
mySimulation <- lapply(1:100, function(i){
  tibble(date = myTestingSet$date,
         run = i,
         value = simulate(object = myTsModel4,
                          xreg = myTestingSet %>% 
                                  select(mean_airtemp, 
                                         mean_windspd, 
                                         max_sun,
                                         mean_soil10,
                                         mean_soil20,
                                         mean_soil50,
                                         mean_soil100)) %>%
                  as.numeric())})
# Combines all tibbles together to form a large tibble
mySimulationAll <- do.call(rbind, mySimulation)
# Calculate the mean of all simulated runs
myTsForecast5 <- mySimulationAll %>% 
                      group_by(date) %>%
                      summarise(fcast = mean(value))
# Visualise the simulated forecast data
ggplot() +
  geom_line(aes(x=date, y=total_demand), myTrainingSet) + 
  geom_line(aes(x=date, y=value, group=run), mySimulationAll, alpha=0.02) + 
  stat_summary(aes(x=date, y=value), mySimulationAll,
               fun.y = mean, 
               geom = "line",
               colour ="blue") +
  labs(x="Date", 
       y="Power Demand")
# Calculate the MSE error
myTestError5 <- mean((myTsForecast5$fcast - myTestingSet$total_demand)^2)
@


At last, you can compare the performance of various models:
<<fig.show='hide', eval=FALSE, results='hide',warning=FALSE, message=FALSE, error=FALSE, size='scriptsize', fig.width=7, fig.height=5>>=
# Linear time series model
myTestError1
# Simple linear regression (not time series model)
myTestError2
# Auto ARIMA model
myTestError3
# ARIMA with custom parameters
myTestError4
# Simulated ARIMA model
myTestError5
@
\end{Exercise}


\chapter{Survival Analysis}

Events occuring at irregular time interval can be studied through survival analysis. It is commonly used to analyse time-to-event in many research areas, such as medicine, economics, engineering and biology. For example, survival analysis is traditionally used in clinical research to analyse the effects of different drugs on sustaining patient's life. In this case, the time to death is used an indicator for drug performance. We will go through several techniques in this chapter.

\section{Kaplan-Meier Estimator}

Kaplan-Meier estimator is used to measure how many subjects survives in a clinical trial since treatment began. At time \(t \leqslant	T\), the estimator is given by equation \eqref{kmestimator} where \(d_{t^{'}}\) represents the number of events and \(n_{t^{'}}\) represents the number of subjects at risk.

\begin{equation}
\label{kmestimator}
\hat{S}_t = \prod_{t^{'}=1}^{t} \Big( 1-\frac{d_{t^{'}}} {n_{t^{'}}} \Big)
\end{equation}


\begin{Exercise}[title={Fitting a Kaplan-Meier Curve}]

There are many implementations for survival analysis in the R language. The most commonly used one is the \verb|survival| package. You can use the \verb|survfit()| function to fit a Kaplan-Meier curve with categorical predictors.

In this exercise, we use the \verb|lung| dataset within the \verb|survival| package which contains lung cancer patients survival time. You can use the command \verb|?lung| to read the detailed dataset description. To fit a Kaplan-Meier Curve, we need to define the target event (i.e. death, in this example) and the time-to-event. Code in example \ref{ex:surv} shows how to define the \verb|Surv| object using the \verb|Surv(time, event)| function. The \verb|survfit| function fits a Kaplan-Meier curve against the target event using the supplied variables.

<<ex:surv, fig.show='hide', eval=FALSE, results='hide',warning=FALSE, message=FALSE, error=FALSE, size='scriptsize', fig.width=7, fig.height=5, cache=TRUE, rexample=TRUE>>=
# Load the survival package for curve fitting
library(survival)
# Use the survminer package for better graphics
library(survminer)
# Load the lungs dataset into current environment
data(lung)
# Read the dataset description
?lung
# Build an empty model
# We are interested in death cases only (status = 2)
# This model has no predictor variable
mySurvFit1 <- survfit(Surv(time, status==2) ~ 1,
                      data = lung)
# Plot the fited curve
ggsurvplot(mySurvFit1)
# Use patient's sex as predictor
mySurvFit2 <- survfit(Surv(time, status==2) ~ sex, 
                      data = lung)
# Plot the curve with confidence interval and p-value
ggsurvplot(mySurvFit2,
           conf.int = TRUE, 
           pval = TRUE)
# The predictor needs to be categorical variable  
# Use age as predictor by encoding into age group categories
mySurvFit3 <- survfit(Surv(time, status==2) ~ cut(age, c(40,50,60,70)), 
                      data = lung)
ggsurvplot(mySurvFit3,pval = TRUE)
@

\begin{figure}[H]
\centering
<<fig.show='asis', echo=FALSE, eval=TRUE, warning=FALSE, message=FALSE, error=FALSE, fig.width=7, fig.height=4>>=
library(survival)
library(survminer)
mySurvFit2 <- survfit(Surv(time, status==2) ~ sex, lung %>% mutate(sex = factor(sex,levels = c(1,2), labels = c("Male","Female"))))
ggsurvplot(mySurvFit2, 
           palette = "Set2",
           conf.int = TRUE,
           risk.table = TRUE,
           risk.table.col = "strata",tables.height = 0.3,
           pval = TRUE)
@
\caption{Kaplan-Meier curves showing two strata}
\end{figure}

\end{Exercise}


\section{Cox Proportional Harzard Model}

To investigate the statistical effects of multiple predictor variables on survival probability, a technique named Cox regression can be used. Cox regression can take into account categorical, ordinal as well as numerical range variables. It analyses the effects of multiple variables on survivial and assumes that the effects of these covariates are time-independent.

The harzard function \(h_t\) is give by equation \eqref{coxph}. The term \(h_{0,t}\) in the equation represents the baseline harzard when all covariates are zero. The linear terms \(x_1, x_2, x_3,..., x_M \) are the predictor variables, while \(\beta_1, \beta_2, \beta_3,..., \beta_M \) are their corresponding coefficients. For each coefficient \(\beta_m\), the exponential term \(e^{\beta_m}\) represents the harzard ratio of the covariate variable \(x_m\). If \(e^{\beta_m} > 1\), the corresponding covariate is positively correlated with increase in hazard. On the other hand, \(x_m\) is negatively correlated with harzard if \(e^{\beta_m} < 1\). In the case where \(e^{\beta_m} = 1\), the variable \(x_m\) has no effects on harzard.

\begin{equation}
\label{coxph}
h_t = h_{0,t} \times e^{\beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 +...+\beta_M x_M}
\end{equation}

Cox model assumes variable effects are time-independent. To test whether the assumptions hold, we can analyse the Schoenfeld residuals for each covariate variable. Equation \eqref{schoenfeld} defines the Schoenfeld residual \(s_{i,k}\) of covariate \(k\) of observation \(i\). It is the difference between covariate \(x_{i,k}\) and the sum of weighted likelihood of failure of all subjects at risks at time \(t\). If there are observable temporal patterns in the residual plot, it suggests the proportional harzard assumptions may have been violated. In this case, you can consider adding interaction effects with time to mitigate the problem.

\begin{equation}
\label{schoenfeld}
s_{i, k} =  x_{i,k} - \sum_{i=1}^{j \in R(t)} x_{i, m} \hat{p_j}
\end{equation}

\begin{Exercise}[title={Training a Cox Regression Model}]

Cox regression model can be trained using the \verb|coxph()| function in the \verb|survival| package. Example \ref{ex:cox} builds a Cox regression model and analyses the effects of different covariate variables on the time-to-death of a group of cancer patients.

<<ex:cox, fig.show='hide', eval=TRUE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize', rexample=TRUE>>=
# Build a Cox model with predictor variables
myCoxModel1 <- coxph(Surv(time, status==2) ~ factor(sex) + age + 
                       ph.ecog + ph.karno + 
                       pat.karno + 
                       meal.cal + wt.loss, data = lung)
# Read the model summary
summary(myCoxModel1)
@

The model output above shows the statistical effects of different covariates. For instance, \verb|sex|\footnote{It is encoded as male=\(1\) and female=\(2\).} is a statistically significant variable for predicting time-to-death of lung cancer patients. This variable has coefficient \(\beta_{sex=2} = \Sexpr{round(myCoxModel1$coefficients['factor(sex)2'],3)} \), which means that having \verb|sex=2| would change the patient's hazard by \(e^{\Sexpr{round(myCoxModel1$coefficients['factor(sex)2'],3)}} - 1=\Sexpr{round((exp(myCoxModel1$coefficients['factor(sex)2']) - 1)*100,1)}\% \). In other words, \verb|sex=2| is benefitial to the patient's wellbeing.

Likewise, \verb|ph.ecog| is also a significant variable. Each unit increase in \verb|ph.ecog| would change the patient's harzard by \( e^{\Sexpr{round(myCoxModel1$coefficients['ph.ecog'],3)}}-1=\Sexpr{round((exp(myCoxModel1$coefficients['ph.ecog']) - 1)*100,1)}\% \). This implies higher \verb|ph.ecog| score significantly increases patient's risk of death.

\end{Exercise}

\begin{Exercise}[title={Cox Regression Diagnostics}]

Example \ref{ex:zph} tests the Cox proportional hazard assumption by calculating the Schoenfeld residuals. If there are observable patterns along the temporal dimension, the model assumptions may have been violated.

<<ex:zph, fig.show='hide', eval=TRUE, results='hide',warning=FALSE, message=FALSE, error=FALSE, size='scriptsize', fig.width=7, fig.height=5, cache=TRUE, rexample=TRUE>>=
# Test the proportional harzard assumption of a Cox regression
myCoxZph1 <- cox.zph(myCoxModel1)
# Print the results of the test 
myCoxZph1
# Plot the Schoenfeld residuals
ggcoxzph(myCoxZph1)
@

\begin{figure}[H]
\centering
<<fig.show='asis', echo=FALSE, eval=TRUE, warning=FALSE, message=FALSE, error=FALSE, fig.width=8, fig.height=6>>=
ggcoxzph(myCoxZph1,var = c("age","ph.ecog","ph.karno"))
@
\caption{Scaled Schoenfeld residuals of a selected set of variables plotted against time}
\end{figure}


\end{Exercise}


\chapter{Unsupervised Learning}

Unsupervised learning identifies the underlying structure of an unlabelled dataset. Clustering is one of the most common applications of unsupervised learning, which aims at allocating similar objects into common groups.

In a given set of unlabelled objects, there can be different ways to produce clusters. Figure \ref{fig:clusters} below shows the effects of choosing differennt number of clusters.

\begin{figure}[H]
\centering
<<fig.show='asis', echo=FALSE, eval=TRUE, warning=FALSE, message=FALSE, error=FALSE, fig.width=7, fig.height=4>>=
# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}

km1 <- iris %>%
  select(Sepal.Length, Sepal.Width) %>%
  kmeans(2)
clust1 <- iris %>% ggplot(aes(x= Sepal.Length, y= Sepal.Width, colour= km1$cluster %>% factor)) +
  geom_point()+
  geom_point(aes(x=Sepal.Length, y=Sepal.Width),km1$centers %>% as_tibble, shape="X",colour="black", size=6)+
  scale_colour_brewer(palette="Set2") +
  theme_classic(base_family = "serif") + 
  theme(legend.position = "none") +
  labs(x="Variable 1", y="Variable 2")

km2 <- iris %>%
  select(Sepal.Length, Sepal.Width) %>%
  kmeans(4)
clust2 <- iris %>% ggplot(aes(x= Sepal.Length, y= Sepal.Width, colour= km2$cluster %>% factor)) +
  geom_point()+
  geom_point(aes(x=Sepal.Length, y=Sepal.Width),km2$centers %>% as_tibble, shape="X",colour="black", size=6)+
  scale_colour_brewer(palette="Set2") +
  theme_classic(base_family = "serif") + 
  theme(legend.position = "none") +
  labs(x="Variable 1", y="Variable 2")

km3 <- iris %>%
  select(Sepal.Length, Sepal.Width) %>%
  kmeans(6)
clust3 <- iris %>% ggplot(aes(x= Sepal.Length, y= Sepal.Width, colour= km3$cluster %>% factor)) +
  geom_point()+
  geom_point(aes(x=Sepal.Length, y=Sepal.Width),km3$centers %>% as_tibble, shape="X",colour="black", size=6)+
  scale_colour_brewer(palette="Set2") +
  theme_classic(base_family = "serif") + 
  theme(legend.position = "none") +
  labs(x="Variable 1", y="Variable 2")

km4 <- iris %>%
  select(Sepal.Length, Sepal.Width) %>%
  kmeans(8)
clust4 <- iris %>% ggplot(aes(x= Sepal.Length, y= Sepal.Width, colour= km4$cluster %>% factor)) +
  geom_point()+
  geom_point(aes(x=Sepal.Length, y=Sepal.Width),km4$centers %>% as_tibble, shape="X",colour="black", size=6)+
  scale_colour_brewer(palette="Set2") +
  theme_classic(base_family = "serif") + 
  theme(legend.position = "none") +
  labs(x="Variable 1", y="Variable 2")

multiplot(clust1, clust2, clust3, clust4,cols = 2)
@
\caption{Different ways to cluster a set of unlabelled objects}
\label{fig:clusters}
\end{figure}

\section{\(K\)-means Clustering}

\(K\)-means clustering a very common clustering algorithm due to its intrinsic simplicity. It produces clusters by minimising the Euclidean distance between objects and the centroid of their own cluster. The Euclidean distance between two \(P\)-dimensional vectors \(\vec{x_j}\) and \(\vec{x_k}\) is defined as equation \eqref{euclidean}.

\begin{equation}
\label{euclidean}
d(\vec{x_j}, \vec{x_k}) = \sqrt{ \sum_{p=1}^{P} (x_{j,p} - x_{k,p} )^2 }
\end{equation}

\begin{algorithm}[H]
	\label{algo:kmeans}
	\caption{\(K\)-means clustering}
	\SetKwInOut{Input}{Input}
	\Input{Set of unlabelled object \( X=\{\vec{x}_1, \vec{x}_2, \vec{x}_3, ... ,\vec{x}_N \}\)}
	\Input{Number of clusters \(K\)}
	\Initialise \\
	\( \{ \vec{\mu}_1, \vec{\mu}_2, \vec{\mu}_3, ..., \vec{\mu}_K  \} \leftarrow Randomise (\{ \vec{x}_1, \vec{x}_2, \vec{x}_3, ... , \vec{x}_N    \}, K), K < N \) \;
	\While{ true } {
	  \For{ \(k \leftarrow \{ 1,2,3,...,K\}\) } {
	    \( \omega_k \leftarrow \{ \} \) \;
	  }
	  \For{ \(n \leftarrow \{ 1,2,3,...,N\}\) } {
	    \( k \leftarrow \aggregate{argmin} {k^{'}} d( \vec{\mu}_{k^{'}}, \vec{x}_n ) , k \in {1,2,3,...,K} \) \;
	    \( \omega_k \leftarrow \omega_k \cup \{ \vec{x}_n  \} \) \;
	  }
	  
	  \For{ \(k \leftarrow \{ 1,2,3,...,K\}\) } {
	    \( \vec{\mu}_k^{'} \leftarrow \frac{1}{|\omega_k|} \sum_{\vec{x} \in \omega_k } \vec{x} \) \;
	  }
    \uIf{ \(\vec{\mu}_k^{'} = \vec{\mu}_k, k={1,2,3,...K}\) }{
      \Break \;
    } \Else{
      \( \vec{\mu}_k = \vec{\mu}_k^{'} \) \;
    }
	}
  \Return Cluster centroids \(  \{ \vec{\mu}_1, \vec{\mu}_2, \vec{\mu}_3, ..., \vec{\mu}_K  \} \) \;
  \Return Object cluster assignment \( \{\omega_1, \omega_2, \omega_3, ..., \omega_K \} \)
\end{algorithm}

The algorithm starts with a randomly select subset of \(K\) objects as initial cluster centroids such as \( \{ \vec{\mu}_1, \vec{\mu}_2, \vec{\mu}_3, ..., \vec{\mu}_K  \} \). The Euclidean distance between initial centroids and each object in the unlabelled set is calculated. The cluster assignment of an object belongs to the centroid with shortest distance. Once cluster assignment is completed for all objects, the centroid is recomputed as the mean of the cluster. This process iterates until the new cluster assignment is identical to the one at the previous iteration.

Since the dataset is unlabelled, the true number of cluster is unknown. The \(K\) value which represents the number of clusters is usually experimented one-by-one and the best value is determined from the output. 

In the R language, the \(K\)-means clustering algorithm is implemented very efficiently. You can use the \verb|kmeans()| function in the \verb|stats| package to perform \(K\)-means clustering.

\begin{Exercise}[title={Dimensionality Reduction}]

In example \ref{ex:pca}, we will use the \verb|mtcars| dataset. This dataset contains six numeric variables. In other words, car can be represented as \(P=6\) dimensional objects. In practical applications of \(K\)-means algorithm, it is very common to normalise the numeric variables using \(z\)-scores if they are recorded in different units. Normalisation would ensure that all variables are fairly represented.

You can use dimensionality reduction techniques such as principal component analysis (PCA) to visualise the data. PCA converts input variables into principal components (PCs) in the order of maximum variance. The following code would execute PCA and visualise the top two PCs on a scatterplot. 

<<ex:pca, fig.show='hide', eval=TRUE, results='hide',warning=FALSE, message=FALSE, error=FALSE, size='scriptsize', fig.width=7, fig.height=5, rexample=TRUE>>=
# Select the numeric variables from the mtcars dataset
mtcars_numeric <- mtcars %>% select(mpg, disp, hp, drat, wt, qsec)
# Calculate the mean and standard deviation for each variables
mtcars_mean <- mtcars_numeric %>% lapply(mean)
mtcars_sd <- mtcars_numeric %>% lapply(sd)
# Convert the numeric variables into z-scores using the mean and sd
mtcars_numeric_normalised <- (mtcars_numeric - mtcars_mean) / mtcars_sd
# There are six variables in this dataset
# We can use principal component analysis (PCA) to reduce the dimensionity
myPca <- prcomp(mtcars_numeric_normalised)
library(ggfortify)
autoplot(myPca, loadings.label = TRUE)
@

\begin{figure}[H]
\centering
<<fig.show='asis', echo=FALSE, eval=TRUE, warning=FALSE, message=FALSE, error=FALSE, fig.width=4, fig.height=3>>=
autoplot(myPca, loadings.label = TRUE) +
  scale_colour_brewer(palette="Set2") +
  theme_classic(base_family = "serif") + 
  theme(legend.position = "none")
@
\caption{Biplot showing the first and second principal components}
\end{figure}

\end{Exercise}


\begin{Exercise}[title={\(K\)-means Clustering}]

With a \(P=6\) dimensional dataset, we can apply \(K\)-means algorithm on it to compute the clusters. Since the number of cluster \(K\) is unknown, we can experiement different values in this example \ref{ex:kmeans}. The clustering results can be visualised in a low-dimensional space with two principal components.

<<ex:kmeans, fig.show='hide', eval=TRUE, results='hide',warning=FALSE, message=FALSE, error=FALSE, size='scriptsize', fig.width=7, fig.height=5, rexample=TRUE>>=
# We know there are three types of of flowers, so let's start with K=3
# You can try different values
myKClust <- kmeans(mtcars_numeric_normalised, centers = 3)
# Visualise the clusters
ggplot(myPca$x, aes(x = PC1,
                    y = PC2,
                    colour = factor(myKClust$cluster))) +
  geom_point() + 
  geom_label(aes(label=mtcars %>% rownames())) + 
  stat_ellipse() +
  labs(colour="Cluster")
@

\begin{figure}[H]
\centering
<<fig.show='asis', echo=FALSE, eval=TRUE, warning=FALSE, message=FALSE, error=FALSE, fig.width=8, fig.height=3>>=
set.seed(1988)
size <- 2
kmPca1 <- kmeans(mtcars_numeric_normalised, centers = 2)$cluster
myClustPlot1 <- ggplot(myPca$x, aes(x = PC1,
                        y = PC2,
                        colour = factor(kmPca1))) +
  geom_point() + 
  geom_label(aes(label=mtcars %>% rownames()), size=size) + 
  stat_ellipse() +
  labs(colour="Cluster") +
  scale_colour_brewer(palette="Set2") +
  theme_classic(base_family = "serif") + 
  theme(legend.position = "bottom")

kmPca2 <- kmeans(mtcars_numeric_normalised, centers = 3)$cluster
myClustPlot2 <- ggplot(myPca$x, aes(x = PC1,
                        y = PC2,
                        colour = factor(kmPca2))) +
  geom_point() + 
  geom_label(aes(label=mtcars %>% rownames()), size=size) + 
  stat_ellipse() +
  labs(colour="Cluster") +
  scale_colour_brewer(palette="Set2") +
  theme_classic(base_family = "serif") + 
  theme(legend.position = "bottom")

kmPca3 <- kmeans(mtcars_numeric_normalised, centers = 4)$cluster
myClustPlot3 <- ggplot(myPca$x, aes(x = PC1,
                        y = PC2,
                        colour = factor(kmPca3))) +
  geom_point() + 
  geom_label(aes(label=mtcars %>% rownames()), size=size) + 
  stat_ellipse() +
  labs(colour="Cluster") +
  scale_colour_brewer(palette="Set2") +
  theme_classic(base_family = "serif") + 
  theme(legend.position = "bottom")

multiplot(myClustPlot1,myClustPlot2,myClustPlot3,cols = 3)
@
\caption{Comparing \(K\)-means clustering results using different \(K\) values}
\end{figure}

\end{Exercise}

\section{Hierarchical Clustering}

In a given unlabelled dataset containing objects \(\vec{x}_i, i=\{1,2,3,...,N\} \), the maximum number of cluster is \(N\) where each cluster contains only \(1\) member object. In this case, the cluster centroids denoted as \( \vec{\mu}_i \) contain rich information which perfectly describes their member objects as \( \vec{\mu}_i = \vec{x}_i \). However, such information would be practically useless. To improve this, we can start with \(N\) clusters and merge the closest two clusters into one. This would have obtained \(N-1\) clusters by adding the least amount of error into the system. This merging process can be iteratively repeated until there are no more clusters left to merge. This is how agglomerative hierarchical clustering algorithm works. 

In hierarchical clustering, the closeness metric between two clusters \( \omega_i \) and \( \omega_j \) is denoted as function \( D(\omega_i,\omega_j) \). There are several common choices of including 
\begin{inparaenum}[1)]
\item single linkage, 
\item complete linkage, 
\item average linkage,
\item centroid and
\item Ward's method
\end{inparaenum}
. For single linkage, the distance between two clusters is defined as the closest distance between their member objects \eqref{singlelinkage}. In many cases, this tends to produce long chains with similar objects merging sequentially into the same cluster. On the other hand, complete linkage uses the distance between farthest objects between two clusters as the cluster closeness metric \eqref{completelinkage}. This tends to produce clusters with consistent size. The two aforementioned measurements are prone to outlier influence. To mitigate such problem, average linkage can be used. It uses the arithmetic average of all pairwise distance as the cluster closeness measurement \eqref{averagelinkage}. Similarly, we can make use of cluster centroid to measure closeness \eqref{centroid}. The centroid method is also resilient to outlier influence. Nonetheless, Ward's method compares the change in sum of squares between cluster members and their centroid when they are merged \eqref{ward}.

\begin{subequations}
	Single linkage
	\begin{equation}
	\label{singlelinkage}
	D(\omega_i, \omega_j) = \min_{\vec{x}_i \in \omega_i, \vec{x}_j \in \omega_j} d(\vec{x}_i, \vec{x}_j)
	\end{equation}
	
	Complete linkage
	\begin{equation}
	\label{completelinkage}
	D(\omega_i, \omega_j) = \max_{\vec{x}_i \in \omega_i, \vec{x}_j \in \omega_j} d(\vec{x}_i, \vec{x}_j)
	\end{equation}
	
	Average linkage
	\begin{equation}
	\label{averagelinkage}
	D(\omega_i, \omega_j) = \underbrace{\frac{1}{|\omega_i|}\frac{1}{|\omega_j|}  \sum_{\vec{x}_i \in \omega_i} \sum_{\vec{x}_j \in \omega_j}  d(\vec{x}_i, \vec{x}_j)}_\text{Average pairwise distance between \(\omega_i\) and \(\omega_j\)}
	\end{equation}
	
	Centroid
	\begin{equation}
	\label{centroid}
	D(\omega_i, \omega_j) = d\Bigg(\Big( \underbrace{\frac{1}{|\omega_i|}\sum_{\vec{x}_i \in \omega_i}\vec{x}_i }_\text{Centroid of \(\omega_i\)} \Big), 
	                               \Big( \underbrace{\frac{1}{|\omega_j|}\sum_{\vec{x}_j \in \omega_j}\vec{x}_j }_\text{Centroid of \(\omega_j\)} \Big) \Bigg)
	\end{equation}
	
	Ward's method
	\begin{equation}
	\label{ward}
  \begin{split}
	D(\omega_i, \omega_j) = 
	        & \underbrace{ \sum_{k \in \omega_i \cup \omega_j } \Big(\vec{x}_k - ( \frac{1}{|\omega_i \cup \omega_j|} \sum_{\vec{x}_{{k}^{'}} \in \omega_i \cup \omega_j} \vec{x}_{{k}^{'}} ) \Big)^2  }_\text{Sum of squares of \(\omega_i \cup \omega_j\)} - \\
        	& \underbrace{ \sum_{i \in \omega_i } \Big(\vec{x}_i - ( \frac{1}{|\omega_i|} \sum_{\vec{x}_{{i}^{'}} \in \omega_i}\vec{x}_{{i}^{'}}) \Big)^2 }_\text{Sum of squares of \(\omega_i\)} - \\
        	& \underbrace{ \sum_{j \in \omega_j } \Big(\vec{x}_j - ( \frac{1}{|\omega_j|} \sum_{\vec{x}_{{j}^{'}} \in \omega_j}\vec{x}_{{j}^{'}}) \Big)^2 }_\text{Sum of squares of \(\omega_j\)}
	\end{split}
	\end{equation}
\end{subequations}


\begin{algorithm}[H]
	\label{algo:hclust}
	\caption{Agglomerative hierarchical clustering}
	\SetKwInOut{Input}{Input}
	\Input{Set of unlabelled object \(X = \{ \vec{x}_1, \vec{x}_2, \vec{x}_3,..., \vec{x}_N\}\)}
	\Input{Linkage function \(D(\omega_i,\omega_j)\)}
	\For{\( n \in \{1,2,3,...,N\} \)}{
	  \( \omega_n \leftarrow \{\vec{x}_n\} \) \;
	}
	\( \Omega \leftarrow \{ \omega_1, \omega_2, \omega_3, ..., \omega_N \} \) \;
	\While{ \( |\Omega| > 1 \) } {
	  \( \Omega^{'} = \{\} \) \;
	  \For{\(i \in \{1,2,3,..., |\Omega|\}\)}{
	    \( \Omega^{'}_i \leftarrow D(\omega_i, \omega_j), j=\{1,2,3,...,|\Omega|\}  \) \;
	  }
	  \( \{i,j\} \leftarrow \aggregate{argmin}{i,j} \Omega^{'} \) \;
	  \( \omega_{ij} \leftarrow \Omega^{'}_i \cup \Omega^{'}_j \) \;
    \( \Omega \leftarrow \Omega^{'} \setminus \Omega^{'}_i \setminus \Omega^{'}_j \cup \omega_{ij} \) \;
	}
\end{algorithm}

The result of hierarchical clustering can be visualised using a tree-like structure called dendrogram. The merging sequence of clusters as well as object closeness can be easily read from the dendrogram. The height of the node at the dendrogram indicates the closeness metric of the two clusters when they are merged. After analysing the dendrogram, users can decide how many clusters to retain. This is usually an objective decision. Once decided, the dendrogram can be cut to obtain the desired number of clusters. Alternatively, we can cut the dendrogram at a certain fixed height to discard trivial clusters at the bottom.

\begin{figure}[H]
\centering
<<fig.show='asis', echo=FALSE, eval=TRUE, warning=FALSE, message=FALSE, error=FALSE, fig.width=8, fig.height=14>>=
StatChull <- ggproto("StatChull", Stat,
  compute_group = function(data, scales) {
    data[chull(data$x, data$y), , drop = FALSE]
  },
  
  required_aes = c("x", "y")
)
stat_chull <- function(mapping = NULL, data = NULL, geom = "polygon",
                       position = "identity", na.rm = FALSE, show.legend = NA, 
                       inherit.aes = TRUE, ...) {
  layer(
    stat = StatChull, data = data, mapping = mapping, geom = geom, 
    position = position, show.legend = show.legend, inherit.aes = inherit.aes,
    params = list(na.rm = na.rm, ...)
  )
}
myDist <- mtcars[,c("disp","qsec")] %>% scale %>% dist()
hc <- myDist %>% hclust()
getHPlot <- function(i){
  ggplot(mtcars, aes(x=disp, y=qsec, group=factor(cutree(hc, k=i)))) +
  geom_point(size=0.5) +
  stat_chull(fill = NA, colour = "red", size=1) +
  #geom_text(aes(x=250, y=22,label=)) +
  scale_colour_brewer(palette="Set2") +
  theme_classic(base_family = "serif") + 
  theme(legend.position = "bottom") +
  labs(x=paste0("Iteration ", nrow(mtcars)-i+1 ), y=NULL) +
  scale_x_continuous(labels = NULL) + 
  scale_y_continuous(labels = NULL)
}
plt <- lapply(32:1, getHPlot)
multiplot(plotlist = plt, cols = 4)
@
\caption{Iterative steps of agglomerative hierarchical clustering}
\end{figure}

\begin{Exercise}[title={Constructing a Dendrogram}]

In the R language, hierarchical clustering can be performed using the \verb|hclust()| function which is included in the default \verb|stats| package. The function requires a distance matrix of objects which is normally pre-computed using the \verb|dist()| function. The \verb|hclust()| function uses complete linkage by default if the \verb|method| parameter is not specified. You can change the linkage function and check the difference in output results. The code snippet in example \ref{ex:hclust} performs hierarchical clustering and visualises the result as a simple dendrogram.

<<ex:hclust, fig.show='hide', eval=TRUE, results='hide',warning=FALSE, message=FALSE, error=FALSE, size='scriptsize', fig.width=7, fig.height=5, rexample=TRUE>>=
# Calculate distance matrix
# Using Euclidean distance here but you can change it
myDist <- mtcars_numeric_normalised %>% dist(method = "euclidean")
# Perform hierarchical clustering using complete linkage
myHClust <- myDist %>% hclust(method = "complete")
# You can change the closeness measurement
# Read the documentation of the hclust function
?hclust
# Visualise the dendrogram
plot(myHClust)
# You can use ggdendrogram to plot a prettier dendrogram
library(ggdendro)
ggdendrogram(myHClust)
@

\begin{figure}[H]
\centering
<<fig.show='asis', echo=FALSE, eval=TRUE, warning=FALSE, message=FALSE, error=FALSE, fig.width=8, fig.height=4>>=
ggdendrogram(myHClust)
@
\caption{Dendrogram illustrating hierarchical clustering using complete linkage}
\end{figure}

\end{Exercise}

\begin{Exercise}[title={Cutting a Dendrogram}]

Dendrogram can be cut to remove smaller clusters at lower height. This can be achieved using the \verb|cutree()| function in R. You can either specify the number of clusters to retain using parameter \verb|k|, this would retain the top \verb|k| clusters of the dendrogram. Alternatively, you can use the \verb|h| parameter to specify at which height the dendrogram should be cut. Example \ref{ex:cutdendro} shows how to cut a dendrogram. It also demonstrates various ways to visualise a dendrogram.

<<ex:cutdendro, fig.show='hide', eval=TRUE, results='hide',warning=FALSE, message=FALSE, error=FALSE, size='scriptsize', fig.width=7, fig.height=5, rexample=TRUE>>=
# Cut the dendrogram by specifying how many clusters to retain
# You can change the number of clusters here
myCutClusters1 <- cutree(myHClust, k = 5) %>% factor()
# Alternatively, cut the dendrogram at a certain height
myCutClusters2 <- cutree(myHClust, h = 6) %>% factor()
# Use the ape package to plot pretty dendrograms
# The RColorBrewer package generates colour palette
library(ape)
library(RColorBrewer)
# Obtain colour definition
myColours <- brewer.pal(n = 5, name="Set1")
# Convert the hierarchical cluster result into a phylogram object
myPhylo <- myHClust %>% as.phylo()
# Draw some plots
# This is a phylogenic tree
plot(myPhylo, 
     type = "phylogram",
     tip.color = myColours[myCutClusters1])
# This is a cladogram
plot(myPhylo, 
     type = "cladogram",
     tip.color = myColours[myCutClusters1])
# This is a unrooted phylogenic tree
plot(myPhylo, 
     type = "unrooted",
     tip.color = myColours[myCutClusters1])
# This is a fan phylogram
plot(myPhylo, 
     type = "fan",
     tip.color = myColours[myCutClusters1])
# This is a radial phylogram
plot(myPhylo, 
     type = "radial",
     tip.color = myColours[myCutClusters1])
@

\begin{figure}[H]
\centering
<<fig.show='asis', echo=FALSE, eval=TRUE, warning=FALSE, message=FALSE, error=FALSE, fig.width=8, fig.height=8>>=
plot(myPhylo, 
     type = "fan",
     tip.color = brewer.pal(5, name="Set2")[myCutClusters1])
@
\caption{Fan phylogram showing hierarchical clusters}
\end{figure}

\end{Exercise}

\chapter{Extending R}

There are many ways to extend \verb|R| functionalities. We will introduce some of them in this chapter.

\section{R Markdown}

\verb|R| Markdown is a way to produce documents dynamically. It supports various output formats such as HTML, PDF, LaTeX and Beamer. The document is self-contained and fully reproducible which makes it easy to share. 

RStudio is integrated with \verb|R| Markdown so that users can manipulate the documents very easily. User can launch a \verb|R| Markdown template by navigating to \textbf{File > New File > R Markdown}. This creates a \verb|.Rmd| template, it contains three types of content:

\begin{description}
  \item[YAML header] Surrounded by \verb|---| at the top of the document. Users can specify key parameters here, such as output format, 
  \item[Text] Standard Markdown format.
  \item[Code chunk] Surrounded by \verb|```|, optional arguments can be provided in the trailing curly bracket \verb|{}|.
\end{description}

A standard YAML header can render basic properties such as document title and author names in the HTML output file. Additional properties can be supplied to modify style and enable advanced features.

The text body is written in standard markdown format. Markdown is a lightweight markup language with plain text formatting syntax. Small pieces of inline \verb|R| code is surrounded by the \verb|`r| mark. This will print the \verb|R| output in the compiled output.

Large chunk of multi-line \verb|R| code is embraced by the \verb|```{r}| symbol. Additional arguments can be passed to each code chunk. For example, \verb|message=FALSE| would supress package loading message. On the other hand, \verb|echo=FALSE| would hide the \verb|R| code while still executing the chunk.

\begin{Exercise}[title={Dynamically Generating a Report}]

This example shows how to dynamically generate HTML docuemnt using \verb|R| Markdown. In RStudio, user can compile a \verb|.Rmd| file by clicking the \textbf{Knit} button or pressing \verb|Ctrl + Shift + K|. This would send the document to the \verb|knitr| engine for compilation. The compiled output is shown in Figure \ref{fig:rmarkdown}.

\begin{verbatim}
---
title: "Car Assessment Report"
author: "James Bond"
output: html_document
---
Vehicle Analysis
=======

I have analysed `r nrow(mtcars)` cars systematically. The 
following vehicles have the largest `horsepower`:

```{r, message=FALSE}
library(dplyr)
mtcars %>%
  mutate(name = rownames(.)) %>%
  arrange(desc(hp)) %>%
  select(name, hp) %>%
  head(2)
```

The following graph shows that these cars have impressive
**horsepower** and **1/4 mile time**.

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.width=8, fig.height=4}
# Create a ggplot
library(ggplot2)
myPlot <- mtcars %>%
  mutate(car_name = rownames(.),
         hp_rank = dense_rank(desc(hp)),
         top2 = hp_rank <= 2) %>%
  ggplot(aes(x=qsec, y=hp, name=car_name)) +
  stat_smooth(method = "lm") +
  geom_point(aes(colour=top2)) +
  labs(x="1/4 Mile Time (Seconds)",
       y="Horsepower",
       colour="Top 2 Horsepower")
# Convert ggplot to plotly
library(plotly)
myPlot %>% ggplotly(tooltip = "name")
```

## Selection Criteria

Essential gadgets are required for Health and Safety reasons:

* Rocket launchers (_front_ and _back_)
* Ejectable seats
* Bulletproof screen

## Results

Despite the vehicle have outstanding performance, they are not
fitted with essential health and safety equipment. As a result,
none of them pass the assessment. 
\end{verbatim}

\begin{figure}[H]
\label{fig:rmarkdown}
\centering
\includegraphics[width=\textwidth]{img/rmarkdown.PNG}
\caption{Compiled R Markdown output in HTML format}
\end{figure}

\end{Exercise}

\subsection{R Notebook}

\verb|R| Notebook is a special type of \verb|R| Markdown document with chunks that can be executed independently and interactively, with output visible immediately beneath the input. 

Users can launch a \verb|R| Notebook template by navigating to \textbf{File > New File > R Notebook}. It has the same \verb|.Rmd| file extension and the Notebook is configured by the \verb|output: html_notebook| parameter in the YAML header.

The \textbf{Preview} button shows the rendered copy of the Notebook. Unlike \textbf{Knit} for standard \verb|R| Markdown documents, \textbf{Preview} does not run any \verb|R| code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.

\section{Shiny Web Application}

\verb|shiny| is a package for creating interactive web applications. Users can use it to integrate statistical models in \verb|R| with dashboard elements such as chart, drop down menu, checkbox, etc. It is a commonly used to disseminate analytical output to non-technical users.

In a \verb|shiny| application, the web page layout is coded in \verb|R| and automatically rendered. There is no need to write any HTML, CSS or JavaScript anymore. This enables users to focus more on analytical elements and develop dashboards rapidly.

User can launch a template for \verb|shiny| web application in RStudio by navigating to \textbf{File > New File > Shiny Web App}. This creates a new \verb|app.R| file\footnote{Do not change the file name, otherwise the system would not recognise it as a shiny web application.}\footnote{User can also create two files ui.R and server.R seperately. This is suitable for more complex application with longer code.}, there are two main components inside the file:

\begin{description}
  \item [ui] This defines the user interface (UI) for the \verb|shiny| application, e.g. the page layout, location of control widgets, page title, etc.
  \item [server] The server-side application which contains the logic. The analytical code is located in this part.
\end{description}

The \verb|ui| of the default \verb|shiny| template is shown in example \ref{ex:shinyui}. The page layout is defined by the \verb|fluidPage()| function. In this example, the \verb|titlePanel()| function defines the page title which is located at the top. The \verb|sidebarLayout()| function divides the screen layout into two unequal parts. The smaller part is defined by \verb|sidebarPanel()| which contains input controls. On the other hand, the larger part is defined by \verb|mainPanel()| which contains the output. 

Many types of control widgets are supported in \verb|shiny|. They are shown in figure \ref{fig:shinyinput}. A detailed description can be found at \url{https://shiny.rstudio.com/tutorial/written-tutorial/lesson3/}.

<<ex:shinyui, fig.show='hide', results='hide', eval=FALSE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize',rexample=TRUE>>=
# Define UI for application that draws a histogram
ui <- fluidPage(
   # Application title
   titlePanel("Old Faithful Geyser Data"),
   # Sidebar with a slider input for number of bins 
   sidebarLayout(
      sidebarPanel(
         sliderInput("bins",
                     "Number of bins:",
                     min = 1,
                     max = 50,
                     value = 30)),
      # Show a plot of the generated distribution
      mainPanel(
         plotOutput("distPlot")
      )
   )
)
@

\begin{figure}[H]
\label{fig:shinyinput}
\centering
\includegraphics[width=\textwidth]{img/shiny-inputs.PNG}
\caption{Control widgets in shiny}
\end{figure}

Example \ref{ex:shinyserver} shows the \verb|server| component in the default \verb|shiny| template. It is defined as a simple function which accepts two arguments \verb|input| and \verb|output|. In the function, named elements can be added to the \verb|output| and visualised through the \verb|ui|. In this case, the function retrieves variable \verb|input$bins| and renders a histogram, which is eventually returned as a graph as the \verb|output$distPlot| element.

<<ex:shinyserver, fig.show='hide', results='hide', eval=FALSE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize',rexample=TRUE>>=
# Define server logic required to draw a histogram
server <- function(input, output) {
   output$distPlot <- renderPlot({
      # generate bins based on input$bins from ui.R
      x    <- faithful[, 2] 
      bins <- seq(min(x), max(x), length.out = input$bins + 1)
      # draw the histogram with the specified number of bins
      hist(x, breaks = bins, col = 'darkgray', border = 'white')
   })
}
@

To run a \verb|shiny| application, user can click on the \textbf|Run App| button in RStudio. This will render the application. Figure \ref{fig:shiny} shows the \verb|shiny| application rendered from the default template.

\begin{figure}[H]
\label{fig:shiny}
\centering
\includegraphics[width=\textwidth]{img/shiny.PNG}
\caption{Default shiny template}
\end{figure}

\begin{Exercise}[title={Design an Analytics Dashboard}]

In this exercise, the objective is to design a simple analytics dashboard which displays flight statistics. Please load the package \verb|nycflights13| and use the \verb|flights| dataset. Steps are indicated in example \ref{ex:shinyloaddata}. 

<<ex:shinyloaddata, fig.show='hide', results='hide', eval=TRUE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize',rexample=TRUE>>=
# Load the package
library(nycflights13)
# Browse the flights dataset
# Aii flights departing from New York airports in 2013.
flights
# Browse the airlines dataset 
# This is a small reference table containing full names of airlines
airlines
@

This is an open-ended creative exercise, the dashboard has to meet the following criteria:

\begin{itemize}
  \item Answers a business questio (user has to define)
  \item Has \textit{at least} two interactive inputs.
  \item Graphically displays \textit{at least} two types of descriptive statistics (e.g. scatterplot, boxplot, bar chart... etc.)
  \item Has a statistical element.
\end{itemize}
\end{Exercise}

To begin with, user can modify the code chunk in example \ref{ex:shinyplotexample}.

<<ex:shinyplotexample, fig.show='asis', results='hide', eval=TRUE, warning=FALSE, message=FALSE, error=FALSE, size='scriptsize',rexample=TRUE>>=
library(dplyr)
library(ggplot2)
library(forcats)
myFlightsPlot <- flights %>%
  mutate(gain = (arr_delay - dep_delay)) %>%
  left_join(airlines, by="carrier") %>% 
  ggplot(aes(x=fct_reorder(name, gain, fun=median, na.rm=TRUE), y=gain)) +
  geom_boxplot() +
  labs(x="Airline", y="Time Gain (Minutes)") +
  coord_flip()
@

\section{Writing Package}


\bibliography{book}{}
\bibliographystyle{plain}

\end{document}